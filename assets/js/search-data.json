{
  
    
        "post0": {
            "title": "Testing CSV Files",
            "content": "import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Atlanta Hawks Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/hawks.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Boston Celtics Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/celtics.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Brooklyn Nets Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/nets.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Charlotte Hornets Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/hornets.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Chicago Bulls Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/bulls.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Cleveland Cavaliers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/cavaliers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Dallas Mavericks Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/mavericks.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Denver Nuggets Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/nuggets.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Detroit Pistons Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/pistons.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Golden State Warriors Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/warriors.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Houston Rockets Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/rockets.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Indiana Pacers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/pacers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Los Angeles Clippers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/clippers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Los Angeles Lakers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/lakers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Memphis Grizzlies Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/grizzlies.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Miami Heat Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/heat.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Milwaukee Bucks Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/bucks.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Minnesota Timberwolves Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/timberwolves.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each New Orleans Pelicans Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/pelicans.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each New York Knicks Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/knicks.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Oklahoma City Thunder Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/thunder.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Orlando Magic Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/magic.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Philadelphia 76ers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/76ers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Phoenix Suns Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/suns.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Portland Trail Blazers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/blazers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Sacramento Kings Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/kings.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each San Antonio Spurs Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/spurs.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Toronto Raptors Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/raptors.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Utah Jazz Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/jazz.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import matplotlib.pyplot as plt import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Washington Wizards Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/wizards.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/05/21/TestingCSVFiles.html",
            "relUrl": "/2023/05/21/TestingCSVFiles.html",
            "date": " • May 21, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Graphing CSV File",
            "content": "import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np . str_filename = &#39;files/basketball.csv&#39; . fh = open(str_filename) csv_reader = csv.reader(fh) . csv_header = next(csv_reader) csv_header . [&#39;Team&#39;, &#39; 2019_Season&#39;, &#39; 2020_Season&#39;, &#39; 2021_Season&#39;, &#39; 2022_Season&#39;, &#39; 2023_Season&#39;] . lst_dt_csv = next(csv_reader) lst_dt_csv . [&#39;Hawks&#39;, &#39; 29&#39;, &#39; 25&#39;, &#39; 47&#39;, &#39; 43&#39;, &#39; 41&#39;] . lst_fgs = next(csv_reader) np_d_fs = np.array(list(map(float, lst_fgs[1:3]))) np_d_fs . array([49., 55.]) . fh.close() . df_sig = pd.read_csv(str_filename, header=None, skiprows=1, names=csv_header) df_sig . Team 2019_Season 2020_Season 2021_Season 2022_Season 2023_Season . 0 Hawks | 29 | 25 | 47 | 43 | 41 | . 1 Celtics | 49 | 55 | 41 | 51 | 57 | . 2 Nets | 42 | 40 | 55 | 44 | 45 | . 3 Hornets | 39 | 29 | 38 | 43 | 27 | . 4 Bulls | 22 | 28 | 35 | 46 | 40 | . 5 Cavaliers | 19 | 24 | 25 | 44 | 51 | . 6 Mavericks | 33 | 47 | 48 | 52 | 38 | . 7 Nuggets | 54 | 52 | 54 | 48 | 53 | . 8 Pistons | 41 | 25 | 23 | 23 | 17 | . 9 Warriors | 57 | 20 | 44 | 53 | 44 | . 10 Rockets | 53 | 50 | 19 | 20 | 22 | . 11 Pacers | 48 | 51 | 39 | 25 | 35 | . 12 Clippers | 48 | 56 | 54 | 42 | 44 | . 13 Lakers | 37 | 60 | 48 | 33 | 43 | . 14 Grizzlies | 33 | 38 | 43 | 56 | 51 | . 15 Heat | 39 | 49 | 46 | 53 | 44 | . 16 Bucks | 60 | 63 | 52 | 51 | 58 | . 17 Timberwolves | 36 | 24 | 26 | 46 | 42 | . 18 Pelicans | 33 | 34 | 35 | 36 | 42 | . 19 Knicks | 17 | 26 | 47 | 37 | 47 | . 20 Thunder | 49 | 50 | 25 | 24 | 40 | . 21 Magic | 42 | 37 | 24 | 22 | 34 | . 22 76ers | 51 | 48 | 56 | 51 | 54 | . 23 Suns | 19 | 38 | 58 | 64 | 45 | . 24 Blazers | 53 | 39 | 48 | 27 | 33 | . 25 Kings | 39 | 35 | 35 | 30 | 48 | . 26 Spurs | 48 | 37 | 38 | 34 | 22 | . 27 Raptors | 58 | 60 | 31 | 48 | 41 | . 28 Jazz | 50 | 50 | 59 | 49 | 37 | . 29 Wizards | 32 | 29 | 39 | 35 | 35 | . #work on later # Import the necessary modules import pandas as pd import numpy as np # Initialize the lists for X and Y data = pd.read_csv(&#39;files/basketball.csv&#39;) x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y1 = list(df.iloc[:, 1]) Y2 = list(df.iloc[:, 2]) Y3 = list(df.iloc[:, 3]) Y4 = list(df.iloc[:, 4]) Y5 = list(df.iloc[:, 5]) # Plot the data using bar() method plt.bar(X, Y1, color=&#39;g&#39;) plt.bar(X, Y2, color=&#39;b&#39;) plt.bar(X, Y3, color=&#39;r&#39;) plt.bar(X, Y4, color=&#39;y&#39;) plt.bar(X, Y5, color=&#39;c&#39;) plt.title(&quot;Wins by Each NBA Team&quot;) plt.xlabel(&quot;Teams&quot;) plt.ylabel(&quot;Number of Wins&quot;) plt.xticks(rotation=90) # Show the plot plt.show() . import pandas as pd # Initialize the lists for X and Y df = pd.read_csv(&#39;files/basketball.csv&#39;) df . Team 2019_Season 2020_Season 2021_Season 2022_Season 2023_Season . 0 Hawks | 29 | 25 | 47 | 43 | 41 | . 1 Celtics | 49 | 55 | 41 | 51 | 57 | . 2 Nets | 42 | 40 | 55 | 44 | 45 | . 3 Hornets | 39 | 29 | 38 | 43 | 27 | . 4 Bulls | 22 | 28 | 35 | 46 | 40 | . 5 Cavaliers | 19 | 24 | 25 | 44 | 51 | . 6 Mavericks | 33 | 47 | 48 | 52 | 38 | . 7 Nuggets | 54 | 52 | 54 | 48 | 53 | . 8 Pistons | 41 | 25 | 23 | 23 | 17 | . 9 Warriors | 57 | 20 | 44 | 53 | 44 | . 10 Rockets | 53 | 50 | 19 | 20 | 22 | . 11 Pacers | 48 | 51 | 39 | 25 | 35 | . 12 Clippers | 48 | 56 | 54 | 42 | 44 | . 13 Lakers | 37 | 60 | 48 | 33 | 43 | . 14 Grizzlies | 33 | 38 | 43 | 56 | 51 | . 15 Heat | 39 | 49 | 46 | 53 | 44 | . 16 Bucks | 60 | 63 | 52 | 51 | 58 | . 17 Timberwolves | 36 | 24 | 26 | 46 | 42 | . 18 Pelicans | 33 | 34 | 35 | 36 | 42 | . 19 Knicks | 17 | 26 | 47 | 37 | 47 | . 20 Thunder | 49 | 50 | 25 | 24 | 40 | . 21 Magic | 42 | 37 | 24 | 22 | 34 | . 22 76ers | 51 | 48 | 56 | 51 | 54 | . 23 Suns | 19 | 38 | 58 | 64 | 45 | . 24 Blazers | 53 | 39 | 48 | 27 | 33 | . 25 Kings | 39 | 35 | 35 | 30 | 48 | . 26 Spurs | 48 | 37 | 38 | 34 | 22 | . 27 Raptors | 58 | 60 | 31 | 48 | 41 | . 28 Jazz | 50 | 50 | 59 | 49 | 37 | . 29 Wizards | 32 | 29 | 39 | 35 | 35 | . import seaborn as sns ax = sns.barplot(data=df) . height = [3, 12, 5, 18, 45] bars = (&quot;very long group name 1&quot;,&quot;very long group name 2&quot;,&quot;very long group name 3&quot;,&quot;very long group name 4&quot;,&quot;very long group name 5&quot;) x_pos = np.arange(len(bars)) # Create bars plt.bar(x_pos, height) # Rotation of the bar names plt.xticks(x_pos, bars, rotation=90) # Custom the subplot layout plt.subplots_adjust(bottom=0.4, top=0.99) # Show graphic plt.show() .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/05/21/PandasNATM.html",
            "relUrl": "/2023/05/21/PandasNATM.html",
            "date": " • May 21, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Testing CSV Files",
            "content": "import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Atlanta Hawks Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/hawks.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Boston Celtics Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/celtics.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Brooklyn Nets Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/nets.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Charlotte Hornets Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/hornets.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Chicago Bulls Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/bulls.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Cleveland Cavaliers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/cavaliers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Dallas Mavericks Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/mavericks.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Denver Nuggets Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/nuggets.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Detroit Pistons Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/pistons.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Golden State Warriors Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/warriors.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Houston Rockets Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/rockets.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Indiana Pacers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/pacers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Los Angeles Clippers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/clippers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Los Angeles Lakers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/lakers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Memphis Grizzlies Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/grizzlies.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Miami Heat Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/heat.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Milwaukee Bucks Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/bucks.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Minnesota Timberwolves Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/timberwolves.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each New Orleans Pelicans Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/pelicans.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each New York Knicks Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/knicks.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Oklahoma City Thunder Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/thunder.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Orlando Magic Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/magic.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Philadelphia 76ers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/76ers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Phoenix Suns Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/suns.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Portland Trail Blazers Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/blazers.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Sacramento Kings Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/kings.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each San Antonio Spurs Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/spurs.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Toronto Raptors Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/raptors.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Utah Jazz Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/jazz.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() . import pandas as pd import matplotlib.pyplot as plt import numpy as np stat = input(&quot;What statistic would you like to analyze today? Games Played, Points, Assists, Rebounds, Steals, Blocks, Field Goal %, or 3 Point %?&quot;) plt.title(str(stat) + &quot; of each Washington Wizards Player&quot;) if stat == &quot;Games Played&quot;: stat = 1 elif stat == &quot;Points&quot;: stat = 2 elif stat == &quot;Assists&quot;: stat = 3 elif stat == &quot;Rebounds&quot;: stat = 4 elif stat == &quot;Steals&quot;: stat = 5 elif stat == &quot;Blocks&quot;: stat = 6 elif stat == &quot;Field Goal %&quot;: stat = 7 elif stat == &quot;3 Point %&quot;: stat = 8 data = pd.read_csv(&#39;files/wizards.csv&#39;) graph = input(&quot;Would you like this data as a Bar Graph or a Pie Chart?&quot;) if graph == &quot;Bar Graph&quot;: x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, stat]) plt.bar(X, Y, color=&#39;g&#39;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) elif graph == &quot;Pie Chart&quot;: plt.pie(data.iloc[:, stat], labels=list(data.iloc[:, 0])) plt.axis(&#39;equal&#39;) plt.show() .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/05/18/TestingCSVFiles.html",
            "relUrl": "/2023/05/18/TestingCSVFiles.html",
            "date": " • May 18, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Testing CSV Files",
            "content": "import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np data = pd.read_csv(&#39;files/celtics.csv&#39;) x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y = list(df.iloc[:, 1]) # Plot the data using bar() method plt.bar(X, Y, color=&#39;g&#39;) plt.title(&quot;Points of each Atlanta Hawks Player&quot;) plt.xlabel(&quot;Players&quot;) plt.ylabel(&quot;Statistics&quot;) plt.xticks(rotation=90) # Show the plot plt.show() .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/05/17/TestingCSVFiles.html",
            "relUrl": "/2023/05/17/TestingCSVFiles.html",
            "date": " • May 17, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "Graphing CSV File",
            "content": "import pandas as pd import csv import matplotlib.pyplot as plt from datetime import datetime import numpy as np . str_filename = &#39;files/basketball.csv&#39; . fh = open(str_filename) csv_reader = csv.reader(fh) . csv_header = next(csv_reader) csv_header . [&#39;Team&#39;, &#39; 2019_Season&#39;, &#39; 2020_Season&#39;, &#39; 2021_Season&#39;, &#39; 2022_Season&#39;, &#39; 2023_Season&#39;] . lst_dt_csv = next(csv_reader) lst_dt_csv . [&#39;Hawks&#39;, &#39; 29&#39;, &#39; 25&#39;, &#39; 47&#39;, &#39; 43&#39;, &#39; 41&#39;] . lst_fgs = next(csv_reader) np_d_fs = np.array(list(map(float, lst_fgs[1:3]))) np_d_fs . array([49., 55.]) . fh.close() . df_sig = pd.read_csv(str_filename, header=None, skiprows=1, names=csv_header) df_sig . Team 2019_Season 2020_Season 2021_Season 2022_Season 2023_Season . 0 Hawks | 29 | 25 | 47 | 43 | 41 | . 1 Celtics | 49 | 55 | 41 | 51 | 57 | . 2 Nets | 42 | 40 | 55 | 44 | 45 | . 3 Hornets | 39 | 29 | 38 | 43 | 27 | . 4 Bulls | 22 | 28 | 35 | 46 | 40 | . 5 Cavaliers | 19 | 24 | 25 | 44 | 51 | . 6 Mavericks | 33 | 47 | 48 | 52 | 38 | . 7 Nuggets | 54 | 52 | 54 | 48 | 53 | . 8 Pistons | 41 | 25 | 23 | 23 | 17 | . 9 Warriors | 57 | 20 | 44 | 53 | 44 | . 10 Rockets | 53 | 50 | 19 | 20 | 22 | . 11 Pacers | 48 | 51 | 39 | 25 | 35 | . 12 Clippers | 48 | 56 | 54 | 42 | 44 | . 13 Lakers | 37 | 60 | 48 | 33 | 43 | . 14 Grizzlies | 33 | 38 | 43 | 56 | 51 | . 15 Heat | 39 | 49 | 46 | 53 | 44 | . 16 Bucks | 60 | 63 | 52 | 51 | 58 | . 17 Timberwolves | 36 | 24 | 26 | 46 | 42 | . 18 Pelicans | 33 | 34 | 35 | 36 | 42 | . 19 Knicks | 17 | 26 | 47 | 37 | 47 | . 20 Thunder | 49 | 50 | 25 | 24 | 40 | . 21 Magic | 42 | 37 | 24 | 22 | 34 | . 22 76ers | 51 | 48 | 56 | 51 | 54 | . 23 Suns | 19 | 38 | 58 | 64 | 45 | . 24 Blazers | 53 | 39 | 48 | 27 | 33 | . 25 Kings | 39 | 35 | 35 | 30 | 48 | . 26 Spurs | 48 | 37 | 38 | 34 | 22 | . 27 Raptors | 58 | 60 | 31 | 48 | 41 | . 28 Jazz | 50 | 50 | 59 | 49 | 37 | . 29 Wizards | 32 | 29 | 39 | 35 | 35 | . #work on later # Import the necessary modules import pandas as pd import numpy as np # Initialize the lists for X and Y data = pd.read_csv(&#39;files/basketball.csv&#39;) x_pos = np.arange(len(data)) df = pd.DataFrame(data) X = list(df.iloc[:, 0]) Y1 = list(df.iloc[:, 1]) Y2 = list(df.iloc[:, 2]) Y3 = list(df.iloc[:, 3]) Y4 = list(df.iloc[:, 4]) Y5 = list(df.iloc[:, 5]) # Plot the data using bar() method plt.bar(X, Y1, color=&#39;g&#39;) plt.bar(X, Y2, color=&#39;b&#39;) plt.bar(X, Y3, color=&#39;r&#39;) plt.bar(X, Y4, color=&#39;y&#39;) plt.bar(X, Y5, color=&#39;c&#39;) plt.title(&quot;Wins by Each NBA Team&quot;) plt.xlabel(&quot;Teams&quot;) plt.ylabel(&quot;Number of Wins&quot;) plt.xticks(rotation=90) # Show the plot plt.show() . import pandas as pd # Initialize the lists for X and Y df = pd.read_csv(&#39;files/basketball.csv&#39;) df . Team 2019_Season 2020_Season 2021_Season 2022_Season 2023_Season . 0 Hawks | 29 | 25 | 47 | 43 | 41 | . 1 Celtics | 49 | 55 | 41 | 51 | 57 | . 2 Nets | 42 | 40 | 55 | 44 | 45 | . 3 Hornets | 39 | 29 | 38 | 43 | 27 | . 4 Bulls | 22 | 28 | 35 | 46 | 40 | . 5 Cavaliers | 19 | 24 | 25 | 44 | 51 | . 6 Mavericks | 33 | 47 | 48 | 52 | 38 | . 7 Nuggets | 54 | 52 | 54 | 48 | 53 | . 8 Pistons | 41 | 25 | 23 | 23 | 17 | . 9 Warriors | 57 | 20 | 44 | 53 | 44 | . 10 Rockets | 53 | 50 | 19 | 20 | 22 | . 11 Pacers | 48 | 51 | 39 | 25 | 35 | . 12 Clippers | 48 | 56 | 54 | 42 | 44 | . 13 Lakers | 37 | 60 | 48 | 33 | 43 | . 14 Grizzlies | 33 | 38 | 43 | 56 | 51 | . 15 Heat | 39 | 49 | 46 | 53 | 44 | . 16 Bucks | 60 | 63 | 52 | 51 | 58 | . 17 Timberwolves | 36 | 24 | 26 | 46 | 42 | . 18 Pelicans | 33 | 34 | 35 | 36 | 42 | . 19 Knicks | 17 | 26 | 47 | 37 | 47 | . 20 Thunder | 49 | 50 | 25 | 24 | 40 | . 21 Magic | 42 | 37 | 24 | 22 | 34 | . 22 76ers | 51 | 48 | 56 | 51 | 54 | . 23 Suns | 19 | 38 | 58 | 64 | 45 | . 24 Blazers | 53 | 39 | 48 | 27 | 33 | . 25 Kings | 39 | 35 | 35 | 30 | 48 | . 26 Spurs | 48 | 37 | 38 | 34 | 22 | . 27 Raptors | 58 | 60 | 31 | 48 | 41 | . 28 Jazz | 50 | 50 | 59 | 49 | 37 | . 29 Wizards | 32 | 29 | 39 | 35 | 35 | . import seaborn as sns ax = sns.barplot(data=df) . height = [3, 12, 5, 18, 45] bars = (&quot;very long group name 1&quot;,&quot;very long group name 2&quot;,&quot;very long group name 3&quot;,&quot;very long group name 4&quot;,&quot;very long group name 5&quot;) x_pos = np.arange(len(bars)) # Create bars plt.bar(x_pos, height) # Rotation of the bar names plt.xticks(x_pos, bars, rotation=90) # Custom the subplot layout plt.subplots_adjust(bottom=0.4, top=0.99) # Show graphic plt.show() .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/05/17/PandasNATM.html",
            "relUrl": "/2023/05/17/PandasNATM.html",
            "date": " • May 17, 2023"
        }
        
    
  
    
        ,"post5": {
            "title": "Pandas Lesson Plan and Hacks",
            "content": "Predictive Analysis . Predictive analysis is the use of statistical, data mining, and machine learning techniques to analyze current and historical data in order to make predictions about future events or behaviors. It involves identifying patterns and trends in data, and then using that information to forecast what is likely to happen in the future. . Predictive analysis is used in a wide range of applications, from forecasting sales and demand, to predicting customer behavior, to detecting fraudulent transactions. It involves collecting and analyzing data from a variety of sources, including historical data, customer data, financial data, and social media data, among others. . The process of predictive analysis typically involves the following steps: . Defining the problem and identifying the relevant data sources | Collecting and cleaning the data | Exploring and analyzing the data to identify patterns and trends | Selecting an appropriate model or algorithm to use for predictions | Training and validating the model using historical data | Using the model to make predictions on new data | Monitoring and evaluating the performance of the model over time | Predictive analysis can help organizations make more informed decisions, improve efficiency, and gain a competitive advantage by leveraging insights from data. . It is most commonly used in Retail, where workers try to predict which products would be most popular and try to advertise those products as much as possible, and also Healthcare, where algorithms analyze patterns and reveal prerequisites for diseases and suggest preventive treatment, predict the results of various treatments and choose the best option for each patient individually, and predict disease outbreaks and epidemics. . Pandas . What is Pandas . Pandas is a Python library used for working with data sets. A python library is something It has functions for analyzing, cleaning, exploring, and manipulating data. . Why Use Pandas? . Pandas allows us to analyze big data and make conclusions based on statistical theories. Pandas can clean messy data sets, and make them readable and relevant.Also it is a part of data analysis, and data manipulation. . What Can Pandas Do? . Pandas gives you answers about the data. Like: . Is there a correlation between two or more columns? | What is average value | Max value | Min value | How to load data | Delete data | Sort Data. | . Pandas are also able to delete rows that are not relevant, or contains wrong values, like empty or NULL values. This is called cleaning the data. . Basics of Pandas. . import pandas as pd # What this does is it calls the python pandas library and this code segment is needed whenever incorporating pandas. . DICTIONARIES AND DATASETS . One way you are able to manipulate a pandas data set is by creating a dictionary and calling it as seen with the dict data 1 and pd.dataframe which is a way to print the set. | . import pandas as pd data1 = { &#39;teams&#39;: [&quot;BARCA&quot;, &quot;REAL&quot;, &quot;ATLETICO&quot;], &#39;standings&#39;: [1, 2, 3] } myvar = pd.DataFrame(data1) print(myvar) . teams standings 0 BARCA 1 1 REAL 2 2 ATLETICO 3 . Indexing and manipulaton of data through lists. . With pandas you can also organize the data which is one of its biggest perks, we call this indexing, this is when we define the first column in a data frame. | . import pandas as pd score = [5/5, 5/5, 1/5] myvar = pd.Series(score, index = [&quot;math&quot;, &quot;science&quot;, &quot;pe&quot;]) print(myvar) . math 1.0 science 1.0 pe 0.2 dtype: float64 . Pandas Classes . Within pandas the library consits of a lot of functions which allow you to manipulate datasets in lists dictionsaries and csv files here are some of the ones we are going to cover (hint: take notes on these) . Series | Index | PeriodIndex | DataframeGroupedBy | Categorical | Time Stamp | . PeriodIndex . This allows for a way to repeat data over time that it occurs as seen from january 2022 to december 2023. You can use Y for years, M for months, and D for days. | . import pandas as pd time = pd.period_range(&#39;2022-01&#39;, &#39;2022-12&#39;, freq=&#39;M&#39;) print(time) . PeriodIndex([&#39;2022-01&#39;, &#39;2022-02&#39;, &#39;2022-03&#39;, &#39;2022-04&#39;, &#39;2022-05&#39;, &#39;2022-06&#39;, &#39;2022-07&#39;, &#39;2022-08&#39;, &#39;2022-09&#39;, &#39;2022-10&#39;, &#39;2022-11&#39;, &#39;2022-12&#39;], dtype=&#39;period[M]&#39;) . POPCORN HACKS . Now implement a way to show a period index from June 2022 to July 2023 in days. | . Dataframe Grouped By . This allows for you to organize your data and calculate the different functions such as | count(): returns the number of non-null values in each group. | sum(): returns the sum of values in each group. | mean(): returns the mean of values in each group. | min(): returns the minimum value in each group. | max(): returns the maximum value in each group. | median(): returns the median of values in each group. | var(): returns the variance of values in each group. | agg(): applies one or more functions to each group and returns a new DataFrame with the results. | . import pandas as pd data = { &#39;Category&#39;: [&#39;E&#39;, &#39;F&#39;, &#39;E&#39;, &#39;F&#39;, &#39;E&#39;, &#39;F&#39;, &#39;E&#39;, &#39;F&#39;], &#39;Value&#39;: [100, 250, 156, 255, 240, 303, 253, 3014] } df = pd.DataFrame(data) grouped = df.groupby(&#39;Category&#39;).#GUESS WHAT THIS WOULD BE IF WE WERE LOOKING FOR COMBINED TOTALS!() print(grouped) . Categorical . This sets up a category for something and puts it within the categories and allows for better orginzation | . import pandas as pd colors = pd.Categorical([&#39;yellow&#39;, &#39;orange&#39;, &#39;blue&#39;, &#39;yellow&#39;, &#39;orange&#39;], categories=[&#39;yellow&#39;, &#39;orange&#39;, &#39;blue&#39;]) print(colors) . Timestamp Class . This allows to display a single time which can be useful when working with datasets that deal with time allowing you to manipulate the time you do something and how you do it. | . import pandas as pd timing = pd.Timestamp(&#39;2023-02-05 02:00:00&#39;) print(#WHAT WOULD THIS BE) . CSV FILES! . A csv file contains data and within pandas you are able to call the function and you are able to manipulate the data with the certain data classes talked about above. | . Name, Position, Average, HR, RBI, OPS, JerseyNumber | Manny Machado, 3B, .298, 32, 102, .897, 13 | Tatis Jr, RF, .281, 42, 97, .975, 23 | Juan Soto, LF, .242, 27, 62, .853, 22 | Xanger Bogaerts, SS, .307, 15, 73, .833, 2 | Nelson Cruz, DH, .234, 10, 64, .651, 32 | Matt Carpenter, DH, .305, 15, 37, 1.138, 14 | Cronezone, 1B, .239, 17, 88, .722, 9 | Ha-Seong Kim, 2B, .251, 11, 59, .708, 7 | Trent Grisham, CF, .184, 17, 53, .626, 1 | Luis Campusano, C, .250, 1, 5, .593, 12 | Austin Nola, C, .251, 4, 40, .649, 26 | Jose Azocar, OF, .257, 0, 10, .630, 28 | . QUESTION: WHAT DO YOU GUYS THINK THE INDEX FOR THIS WOULD BE? . Popcorn Hacks . Can you explain what is going on in this code segment below. (hint: define what ascending= false means, and df. head means) | . import pandas as pd #read csv and sort &#39;Duration&#39; largest to smallest df = pd.read_csv(&#39;files/padres.csv&#39;).sort_values(by=[&#39;Name&#39;], ascending=False) print(&quot;--Duration Top 10&quot;) print(df.head(10)) print(&quot;--Duration Bottom 10&quot;) print(df.tail(10)) print(&#39;, &#39;.join(df.tail(10))) . import pandas as pd df = pd.read_csv(&quot;./files/housing.csv&quot;) mode_total_rooms = df[&#39;total_rooms&#39;].mode() print(f&quot;The mode of the &#39;total_rooms&#39; column is: {mode_total_rooms}&quot;) . import pandas as pd df = pd.read_csv(&quot;./files/housing.csv&quot;) grouped_df = df.groupby(&#39;total_rooms&#39;) agg_df = grouped_df.agg({&#39;total_rooms&#39;: &#39;sum&#39;, &#39;population&#39;: &#39;mean&#39;, &#39;longitude&#39;: &#39;count&#39;}) # WHAT DO YOU GUYS THINK df.agg means in context of pandas and what does it stand for. print(agg_df) . total_rooms population longitude total_rooms 2.0 2.0 6.0 1 6.0 6.0 8.0 1 8.0 8.0 13.0 1 11.0 11.0 24.0 1 12.0 12.0 18.0 1 ... ... ... ... 30450.0 30450.0 9419.0 1 32054.0 32054.0 15507.0 1 32627.0 32627.0 28566.0 1 37937.0 37937.0 16122.0 1 39320.0 39320.0 16305.0 1 [5926 rows x 3 columns] . Hacks . Take notes on content in the notebook file. | FIll IN THE BLANKS FOR THE CODE SEGMENTS THAT NEED IT? | Research another type of class that is used in pandas | Implement that class with pandas. as seen with the examples above | .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/04/25/PandasHacks.html",
            "relUrl": "/2023/04/25/PandasHacks.html",
            "date": " • Apr 25, 2023"
        }
        
    
  
    
        ,"post6": {
            "title": "Petite Pandas Data Analysis using Pandas and NumPy",
            "content": "Predictive Analysis . Predictive analysis is the use of _, data mining, and machine learning techniques to analyze current and historical data in order to make predictions about future events or behaviors. It involves identifying __ and trends in data, and then using that information to forecast what is likely to happen in the future. . Predictive analysis is used in a wide range of applications, from forecasting sales and demand, to predicting customer behavior, to detecting fraudulent transactions. It involves collecting and analyzing data from a variety of sources, including historical data, customer data, financial data, and social media data, among others. . The process of predictive analysis typically involves the following steps: . Defining the problem and identifying the relevant data sources | . | Exploring and analyzing the data to identify patterns and trends | Selecting an appropriate model or algorithm to use for predictions | Training and validating the model using historical data | . | Monitoring and evaluating the performance of the model over time | Predictive analysis can help organizations make more informed decisions, improve efficiency, and gain a competitive advantage by leveraging insights from data. . It is most commonly used in __, where workers try to predict which products would be most popular and try to advertise those products as much as possible, and also ___, where algorithms analyze patterns and reveal prerequisites for diseases and suggest preventive treatment, predict the results of various treatments and choose the best option for each patient individually, and predict disease outbreaks and epidemics. . 1. Intro to NumPy and the features it consists . Numpy, by definition, is the fundamental package for __ in Python which can be used to perform __, provide __, and makes __ much __. Numpy is very important and useful when it comes to data analysis, as it can easily use its features to complete and perform any mathematical operation, as well as analyze data files. . If you don&#39;t already have numpy installed, you can do so using conda install numpy or pip install numpy . Once that is complete, to import numpy in your code, all you must do is: . import numpy as np . 2. Using NumPy to create arrays . An array is the central __ of the NumPy library. They are used as __ which are able to store more than one item at the same time. Using the function np.array is used to create an array, in which you can create multidimensional arrays. . Shown below is how to create a 1D array: . a = np.array([1, 2, 3]) print(a) # this creates a 1D array . How could you create a 3D array based on knowing how to make a 1D array? . . Arrays can be printed in different ways, especially a more readable format. As we have seen, arrays are printed in rows and columns, but we can change that by using the reshape function . c = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) print(c.reshape(1, 9)) # organizes it all in a single line of output . In the code segment below, we can also specially select certain rows and columns from the array to further analyze selective data. . print(c[1:, :2]) # the 1: means &quot;start at row 1 and select all the remaining rows&quot; # the :2 means &quot;select the first two columns&quot; . 3. Basic array operations . One of the most basic operations that can be performed on arrays is ___. With numpy, it is very easy to perform arithmetic operations on arrays. You can __, _, __ and __ arrays, just like you would with regular numbers. When performing these operations, numpy applies the operation element-wise, meaning that it performs the operation on each element in the array separately. This makes it easy to perform operations on large amounts of data quickly and efficiently. . a = np.array([1, 2, 3]) b = np.array([4, 5, 6]) print(a + b) # adds each value based on the column the integer is in print(a - b) # subtracts each value based on the column the integer is in print(a * b) # multiplies each value based on the column the integer is in print(a / b) # divides each value based on the column the integer is in . d = np.exp(b) e = np.sqrt(b) print(d) print(e) . From the knowledge of how to use more advanced mathematical expressions than the basic 4 mathematical operations such as exponent and square root, now can you code how to calculate the 3 main trig expressions (sin, cos, tan), natural log, and log10 of a 1D array. . # calculate cos # calculate tan # calculate natural log # calculate log10 . 4. Data analysis using numpy . Numpy provides a convenient and powerful way to perform data analysis tasks on __. One of the most common tasks in data analysis is finding the __, __, and __ of a dataset. Numpy provides functions to perform these operations quickly and easily. The mean function calculates the average value of the data, while the median function calculates the middle value in the data. The standard deviation function calculates how spread out the data is from the mean. Additionally, numpy provides functions to find the minimum and maximum values in the data. These functions are very useful for gaining insight into the properties of large datasets and can be used for a wide range of data analysis tasks. . data = np.array([2, 5, 12, 13, 19]) print(np.mean(data)) # finds the mean of the dataset print(np.median(data)) # finds the median of the dataset print(np.std(data)) # finds the standard deviation of the dataset print(np.min(data)) # finds the min of the dataset print(np.max(data)) # finds the max of the dataset . Now from learning this, can you find a different way from how we can solve the sum or products of a dataset other than how we learned before? . . Numpy also has the ability to handle CSV files, which are commonly used to store and exchange large datasets. By importing CSV files into numpy arrays, we can easily perform complex operations and analysis on the data, making numpy an essential tool for data scientists and researchers. . genfromtxt and loadtxt are two functions in the numpy library that can be used to read data from text files, including CSV files. . genfromtxt is a more advanced function that can be used to read text files that have more complex structures, including CSV files. genfromtxt can handle files that have missing or invalid data, or files that have columns of different data types. It can also be used to skip header lines or to read only specific columns from the file. . import numpy as np padres = np.genfromtxt(&#39;files/padres.csv&#39;, delimiter=&#39;,&#39;, dtype=str, encoding=&#39;utf-8&#39;) # delimiter indicates that the data is separated into columns which is distinguished by commas # genfromtxt is used to read the csv file itself # dtype is used to have numpy automatically detect the data type in the csv file print(padres) . loadtxt is a simpler function that can be used to read simple text files that have a regular structure, such as files that have only one type of data (such as all integers or all floats). loadtxt can be faster than genfromtxt because it assumes that the data in the file is well-structured and can be easily parsed. . import numpy as np padres = np.loadtxt(&#39;files/padres.csv&#39;, delimiter=&#39;,&#39;, dtype=str, encoding=&#39;utf-8&#39;) print(padres) . for i in padres: print(&quot;,&quot;.join(i)) . Pandas . What is Pandas . Pandas is a Python library used for working with data sets. A python library is something It has functions for analyzing, cleaning, exploring, and manipulating data. . Why Use Pandas? . Pandas allows us to analyze big data and make conclusions based on statistical theories. Pandas can clean messy data sets, and make them readable and relevant. Also it is a part of data analysis, and data manipulation. . What Can Pandas Do? . Pandas gives you answers about the data. Like: . Is there a correlation between two or more columns? | What is average value | Max value | Min value | How to load data | Delete data | Sort Data. | . Pandas are also able to delete rows that are not relevant, or contains wrong values, like empty or NULL values. This is called cleaning the data. . Basics of Pandas. . import pandas as pd # What this does is it calls the python pandas library and this code segment is needed whenever incorporating pandas. . DICTIONARIES AND DATASETS . One way you are able to manipulate a pandas data set is by creating a dictionary and calling it as seen with the dict data 1 and pd.dataframe which is a way to print the set. | . import pandas as pd data1 = { &#39;teams&#39;: [&quot;BARCA&quot;, &quot;REAL&quot;, &quot;ATLETICO&quot;], &#39;standings&#39;: [1, 2, 3] } myvar = pd.DataFrame(data1) print(myvar) . Indexing and manipulaton of data through lists. . With pandas you can also organize the data which is one of its biggest perks, we call this indexing, this is when we define the first column in a data frame. | . import pandas as pd score = [5/5, 5/5, 1/5] myvar = pd.Series(score, index = [&quot;math&quot;, &quot;science&quot;, &quot;pe&quot;]) print(myvar) . Pandas Classes . Within pandas the library consits of a lot of functions which allow you to manipulate datasets in lists dictionsaries and csv files here are some of the ones we are going to cover (hint: take notes on these) . Series | Index | PeriodIndex | DataframeGroupedBy | Categorical | Time Stamp | . PeriodIndex . This allows for a way to repeat data over time that it occurs as seen from january 2022 to december 2023. You can use Y for years, M for months, and D for days. | . import pandas as pd time = pd.period_range(&#39;2022-01&#39;, &#39;2022-12&#39;, freq=&#39;M&#39;) print(time) . Now implement a way to show a period index from June 2022 to July 2023 in days. . . Dataframe Grouped By . This allows for you to organize your data and calculate the different functions such as | count(): returns the number of non-null values in each group. | sum(): returns the sum of values in each group. | mean(): returns the mean of values in each group. | min(): returns the minimum value in each group. | max(): returns the maximum value in each group. | median(): returns the median of values in each group. | var(): returns the variance of values in each group. | agg(): applies one or more functions to each group and returns a new DataFrame with the results. | . import pandas as pd data = { &#39;Category&#39;: [&#39;E&#39;, &#39;F&#39;, &#39;E&#39;, &#39;F&#39;, &#39;E&#39;, &#39;F&#39;, &#39;E&#39;, &#39;F&#39;], &#39;Value&#39;: [100, 250, 156, 255, 240, 303, 253, 3014] } df = pd.DataFrame(data) grouped = df.groupby(&#39;Category&#39;).#GUESS WHAT THIS WOULD BE IF WE WERE LOOKING FOR COMBINED TOTALS!() print(grouped) . Categorical . This sets up a category for something and puts it within the categories and allows for better orginzation | . import pandas as pd colors = pd.Categorical([&#39;yellow&#39;, &#39;orange&#39;, &#39;blue&#39;, &#39;yellow&#39;, &#39;orange&#39;], categories=[&#39;yellow&#39;, &#39;orange&#39;, &#39;blue&#39;]) print(colors) . Timestamp Class . This allows to display a single time which can be useful when working with datasets that deal with time allowing you to manipulate the time you do something and how you do it. | . import pandas as pd timing = pd.Timestamp(&#39;2023-02-05 02:00:00&#39;) print(#WHAT WOULD THIS BE) . CSV FILES! . A csv file contains data and within pandas you are able to call the function and you are able to manipulate the data with the certain data classes talked about above. | . Name, Position, Average, HR, RBI, OPS, JerseyNumber | Manny Machado, 3B, .298, 32, 102, .897, 13 | Tatis Jr, RF, .281, 42, 97, .975, 23 | Juan Soto, LF, .242, 27, 62, .853, 22 | Xanger Bogaerts, SS, .307, 15, 73, .833, 2 | Nelson Cruz, DH, .234, 10, 64, .651, 32 | Matt Carpenter, DH, .305, 15, 37, 1.138, 14 | Cronezone, 1B, .239, 17, 88, .722, 9 | Ha-Seong Kim, 2B, .251, 11, 59, .708, 7 | Trent Grisham, CF, .184, 17, 53, .626, 1 | Luis Campusano, C, .250, 1, 5, .593, 12 | Austin Nola, C, .251, 4, 40, .649, 26 | Jose Azocar, OF, .257, 0, 10, .630, 28 | . QUESTION: WHAT DO YOU GUYS THINK THE INDEX FOR THIS WOULD BE? . Can you explain what is going on in this code segment below. (hint: define what ascending= false means, and df. head means) . import pandas as pd #read csv and sort &#39;Duration&#39; largest to smallest df = pd.read_csv(&#39;files/padres.csv&#39;).sort_values(by=[&#39;Name&#39;], ascending=False) print(&quot;--Duration Top 10&quot;) print(df.head(10)) print(&quot;--Duration Bottom 10&quot;) print(df.tail(10)) print(&#39;, &#39;.join(df.tail(10))) . import pandas as pd df = pd.read_csv(&quot;./files/housing.csv&quot;) mode_total_rooms = df[&#39;total_rooms&#39;].mode() print(f&quot;The mode of the &#39;total_rooms&#39; column is: {mode_total_rooms}&quot;) . import pandas as pd df = pd.read_csv(&quot;./files/housing.csv&quot;) grouped_df = df.groupby(&#39;total_rooms&#39;) agg_df = grouped_df.agg({&#39;total_rooms&#39;: &#39;sum&#39;, &#39;population&#39;: &#39;mean&#39;, &#39;longitude&#39;: &#39;count&#39;}) # WHAT DO YOU GUYS THINK df.agg means in context of pandas and what does it stand for. print(agg_df) . Our Frontend Data Analysis Project . Link . Popcorn Hacks . Complete fill in the blanks for Predictive Analysis Numpy | Takes notes on Panda where it asks you to | Complete code segment tasks in Panda and Numpy | . Main Hack . Make a data file - content is up to you, just make sure there are integer values - and print | Run Panda and Numpy commands Panda: Find Min and Max values | Sort in order - can be order of least to greatest or vice versa | Create a smaller dataframe and merge it with your data file | . | Numpy: Random number generation | create a multi-dimensional array (multiple elements) | create an array with linearly spaced intervals between values | . | . | . Grading . The grading will be binary - all or nothing; no partial credit . 0.3 for all the popcorn hacks | 0.6 for the main hack - CSV file | 0.1 for going above and beyond in the main hack | .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/04/25/Data-Analysis-Panda-Numpy.html",
            "relUrl": "/2023/04/25/Data-Analysis-Panda-Numpy.html",
            "date": " • Apr 25, 2023"
        }
        
    
  
    
        ,"post7": {
            "title": "P5-Y 4/25 Data Analysis with Numpy & Pandas",
            "content": "WGET links . CSV File (add to /_notebooks/files/) | Second CSV File (add to /_notebooks/files/) | Data Analysis Lesson + Hacks | . Predictive Analysis . Predictive analysis is the use of statistical, data mining, and machine learning techniques to analyze current and historical data in order to make predictions about future events or behaviors. It involves identifying patterns and trends in data, and then using that information to forecast what is likely to happen in the future. . Predictive analysis is used in a wide range of applications, from forecasting sales and demand, to predicting customer behavior, to detecting fraudulent transactions. It involves collecting and analyzing data from a variety of sources, including historical data, customer data, financial data, and social media data, among others. . The process of predictive analysis typically involves the following steps: . Defining the problem and identifying the relevant data sources | Collecting and cleaning the data | Exploring and analyzing the data to identify patterns and trends | Selecting an appropriate model or algorithm to use for predictions | Training and validating the model using historical data | Using the model to make predictions on new data | Monitoring and evaluating the performance of the model over time | Predictive analysis can help organizations make more informed decisions, improve efficiency, and gain a competitive advantage by leveraging insights from data. . It is most commonly used in Retail, where workers try to predict which products would be most popular and try to advertise those products as much as possible, and also Healthcare, where algorithms analyze patterns and reveal prerequisites for diseases and suggest preventive treatment, predict the results of various treatments and choose the best option for each patient individually, and predict disease outbreaks and epidemics. . 1. Intro to NumPy and the features it consists . Numpy, by definition, is the fundamental package for scientific computing in Python which can be used to perform mathematical operations, providing multidimensional array objects, and makes data analysis much easier. Numpy is very important and useful when it comes to data analysis, as it can easily use its features to complete and perform any mathematical operation, as well as analyze data files. . If you don&#39;t already have numpy installed, you can do so using conda install numpy or pip install numpy . Once that is complete, to import numpy in your code, all you must do is: . import numpy as np . 2. Using NumPy to create arrays . An array is the central data structure of the NumPy library. They are used as containers which are able to store more than one item at the same time. Using the function np.array is used to create an array, in which you can create multidimensional arrays. . Shown below is how to create a 1D array: . a = np.array([1, 2, 3]) print(a) # this creates a 1D array . How could you create a 3D array based on knowing how to make a 1D array? . . Arrays can be printed in different ways, especially a more readable format. As we have seen, arrays are printed in rows and columns, but we can change that by using the reshape function . c = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) print(c.reshape(1, 9)) # organizes it all in a single line of output . In the code segment below, we can also specially select certain rows and columns from the array to further analyze selective data. . print(c[1:, :2]) # the 1: means &quot;start at row 1 and select all the remaining rows&quot; # the :2 means &quot;select the first two columns&quot; . 3. Basic array operations . One of the most basic operations that can be performed on arrays is arithmetic operations. With numpy, it is very easy to perform arithmetic operations on arrays. You can add, subtract, multiply and divide arrays, just like you would with regular numbers. When performing these operations, numpy applies the operation element-wise, meaning that it performs the operation on each element in the array separately. This makes it easy to perform operations on large amounts of data quickly and efficiently. . a = np.array([1, 2, 3]) b = np.array([4, 5, 6]) print(a + b) # adds each value based on the column the integer is in print(a - b) # subtracts each value based on the column the integer is in print(a * b) # multiplies each value based on the column the integer is in print(a / b) # divides each value based on the column the integer is in . d = np.exp(b) e = np.sqrt(b) print(d) print(e) . From the knowledge of how to use more advanced mathematical expressions than the basic 4 mathematical operations such as exponent and square root, now can you code how to calculate the 3 main trig expressions (sin, cos, tan), natural log, and log10 of a 1D array. . # calculate cos # calculate tan # calculate natural log # calculate log10 . 4. Data analysis using numpy . Numpy provides a convenient and powerful way to perform data analysis tasks on large datasets. One of the most common tasks in data analysis is finding the mean, median, and standard deviation of a dataset. Numpy provides functions to perform these operations quickly and easily. The mean function calculates the average value of the data, while the median function calculates the middle value in the data. The standard deviation function calculates how spread out the data is from the mean. Additionally, numpy provides functions to find the minimum and maximum values in the data. These functions are very useful for gaining insight into the properties of large datasets and can be used for a wide range of data analysis tasks. . data = np.array([2, 5, 12, 13, 19]) print(np.mean(data)) # finds the mean of the dataset print(np.median(data)) # finds the median of the dataset print(np.std(data)) # finds the standard deviation of the dataset print(np.min(data)) # finds the min of the dataset print(np.max(data)) # finds the max of the dataset . Now from learning this, can you find a different way from how we can solve the sum or products of a dataset other than how we learned before? . . Numpy also has the ability to handle CSV files, which are commonly used to store and exchange large datasets. By importing CSV files into numpy arrays, we can easily perform complex operations and analysis on the data, making numpy an essential tool for data scientists and researchers. . genfromtxt and loadtxt are two functions in the numpy library that can be used to read data from text files, including CSV files. . genfromtxt is a more advanced function that can be used to read text files that have more complex structures, including CSV files. genfromtxt can handle files that have missing or invalid data, or files that have columns of different data types. It can also be used to skip header lines or to read only specific columns from the file. . import numpy as np padres = np.genfromtxt(&#39;files/padres.csv&#39;, delimiter=&#39;,&#39;, dtype=str, encoding=&#39;utf-8&#39;) # delimiter indicates that the data is separated into columns which is distinguished by commas # genfromtxt is used to read the csv file itself # dtype is used to have numpy automatically detect the data type in the csv file print(padres) . loadtxt is a simpler function that can be used to read simple text files that have a regular structure, such as files that have only one type of data (such as all integers or all floats). loadtxt can be faster than genfromtxt because it assumes that the data in the file is well-structured and can be easily parsed. . import numpy as np padres = np.loadtxt(&#39;files/padres.csv&#39;, delimiter=&#39;,&#39;, dtype=str, encoding=&#39;utf-8&#39;) print(padres) . for i in padres: print(&quot;,&quot;.join(i)) . Pandas . What is Pandas . Pandas is a Python library used for working with data sets. A python library is something It has functions for analyzing, cleaning, exploring, and manipulating data. . Why Use Pandas? . Pandas allows us to analyze big data and make conclusions based on statistical theories. Pandas can clean messy data sets, and make them readable and relevant. Also it is a part of data analysis, and data manipulation. . What Can Pandas Do? . Pandas gives you answers about the data. Like: . Is there a correlation between two or more columns? | What is average value | Max value | Min value | How to load data | Delete data | Sort Data. | . Pandas are also able to delete rows that are not relevant, or contains wrong values, like empty or NULL values. This is called cleaning the data. . Basics of Pandas. . import pandas as pd # What this does is it calls the python pandas library and this code segment is needed whenever incorporating pandas. . DICTIONARIES AND DATASETS . One way you are able to manipulate a pandas data set is by creating a dictionary and calling it as seen with the dict data 1 and pd.dataframe which is a way to print the set. | . import pandas as pd data1 = { &#39;teams&#39;: [&quot;BARCA&quot;, &quot;REAL&quot;, &quot;ATLETICO&quot;], &#39;standings&#39;: [1, 2, 3] } myvar = pd.DataFrame(data1) print(myvar) . Indexing and manipulaton of data through lists. . With pandas you can also organize the data which is one of its biggest perks, we call this indexing, this is when we define the first column in a data frame. | . import pandas as pd score = [5/5, 5/5, 1/5] myvar = pd.Series(score, index = [&quot;math&quot;, &quot;science&quot;, &quot;pe&quot;]) print(myvar) . Pandas Classes . Within pandas the library consits of a lot of functions which allow you to manipulate datasets in lists dictionsaries and csv files here are some of the ones we are going to cover (hint: take notes on these) . Series | Index | PeriodIndex | DataframeGroupedBy | Categorical | Time Stamp | . PeriodIndex . This allows for a way to repeat data over time that it occurs as seen from january 2022 to december 2023. You can use Y for years, M for months, and D for days. | . import pandas as pd time = pd.period_range(&#39;2022-01&#39;, &#39;2022-12&#39;, freq=&#39;M&#39;) print(time) . Now implement a way to show a period index from June 2022 to July 2023 in days. . . Dataframe Grouped By . This allows for you to organize your data and calculate the different functions such as | count(): returns the number of non-null values in each group. | sum(): returns the sum of values in each group. | mean(): returns the mean of values in each group. | min(): returns the minimum value in each group. | max(): returns the maximum value in each group. | median(): returns the median of values in each group. | var(): returns the variance of values in each group. | agg(): applies one or more functions to each group and returns a new DataFrame with the results. | . import pandas as pd data = { &#39;Category&#39;: [&#39;E&#39;, &#39;F&#39;, &#39;E&#39;, &#39;F&#39;, &#39;E&#39;, &#39;F&#39;, &#39;E&#39;, &#39;F&#39;], &#39;Value&#39;: [100, 250, 156, 255, 240, 303, 253, 3014] } df = pd.DataFrame(data) grouped = df.groupby(&#39;Category&#39;).#GUESS WHAT THIS WOULD BE IF WE WERE LOOKING FOR COMBINED TOTALS!() print(grouped) . Categorical . This sets up a category for something and puts it within the categories and allows for better orginzation | . import pandas as pd colors = pd.Categorical([&#39;yellow&#39;, &#39;orange&#39;, &#39;blue&#39;, &#39;yellow&#39;, &#39;orange&#39;], categories=[&#39;yellow&#39;, &#39;orange&#39;, &#39;blue&#39;]) print(colors) . Timestamp Class . This allows to display a single time which can be useful when working with datasets that deal with time allowing you to manipulate the time you do something and how you do it. | . import pandas as pd timing = pd.Timestamp(&#39;2023-02-05 02:00:00&#39;) print(#WHAT WOULD THIS BE) . CSV FILES! . A csv file contains data and within pandas you are able to call the function and you are able to manipulate the data with the certain data classes talked about above. | . Name, Position, Average, HR, RBI, OPS, JerseyNumber | Manny Machado, 3B, .298, 32, 102, .897, 13 | Tatis Jr, RF, .281, 42, 97, .975, 23 | Juan Soto, LF, .242, 27, 62, .853, 22 | Xanger Bogaerts, SS, .307, 15, 73, .833, 2 | Nelson Cruz, DH, .234, 10, 64, .651, 32 | Matt Carpenter, DH, .305, 15, 37, 1.138, 14 | Cronezone, 1B, .239, 17, 88, .722, 9 | Ha-Seong Kim, 2B, .251, 11, 59, .708, 7 | Trent Grisham, CF, .184, 17, 53, .626, 1 | Luis Campusano, C, .250, 1, 5, .593, 12 | Austin Nola, C, .251, 4, 40, .649, 26 | Jose Azocar, OF, .257, 0, 10, .630, 28 | . QUESTION: WHAT DO YOU GUYS THINK THE INDEX FOR THIS WOULD BE? . Can you explain what is going on in this code segment below. (hint: define what ascending= false means, and df. head means) . import pandas as pd #read csv and sort &#39;Duration&#39; largest to smallest df = pd.read_csv(&#39;files/padres.csv&#39;).sort_values(by=[&#39;Name&#39;], ascending=False) print(&quot;--Duration Top 10&quot;) print(df.head(10)) print(&quot;--Duration Bottom 10&quot;) print(df.tail(10)) print(&#39;, &#39;.join(df.tail(10))) . import pandas as pd df = pd.read_csv(&quot;./files/housing.csv&quot;) mode_total_rooms = df[&#39;total_rooms&#39;].mode() print(f&quot;The mode of the &#39;total_rooms&#39; column is: {mode_total_rooms}&quot;) . import pandas as pd df = pd.read_csv(&quot;./files/housing.csv&quot;) grouped_df = df.groupby(&#39;total_rooms&#39;) agg_df = grouped_df.agg({&#39;total_rooms&#39;: &#39;sum&#39;, &#39;population&#39;: &#39;mean&#39;, &#39;longitude&#39;: &#39;count&#39;}) # WHAT DO YOU GUYS THINK df.agg means in context of pandas and what does it stand for. print(agg_df) . Project . Link . Popcorn Hacks . Complete fill in the blanks for Predictive Analysis and Numpy lessons | Takes notes on Panda lesson where it asks you to | Complete code segment tasks in Panda and Numpy lessons | Answer the questions in Panda lesson | . Main Hack . Make a data file - content is up to you, just make sure there are integer values - and print | Run Panda and Numpy commands Panda: Find Min and Max values | Sort in order - can be order of least to greatest or vice versa | Create a smaller dataframe and merge it with your data file | . | Numpy: Random number generation | create a multi-dimensional array (multiple elements) | create an array with linearly spaced intervals between values | . | . | . Grading . The grading will be binary - all or nothing; no partial credit . 0.3 for all the popcorn hacks | 0.6 for the main hack - CSV file | 0.1 for going above and beyond in the main hack | .",
            "url": "https://paravsalaniwal.github.io/T3Project/student/2023/04/25/Combined-Lesson-Plan.html",
            "relUrl": "/student/2023/04/25/Combined-Lesson-Plan.html",
            "date": " • Apr 25, 2023"
        }
        
    
  
    
        ,"post8": {
            "title": "Test",
            "content": "import numpy as np . padres = np.loadtxt(&#39;files/padres.csv&#39;, delimiter=&#39;,&#39;, dtype=str, encoding=&#39;utf-8&#39;) print(padres) . [[&#39;Name&#39; &#39; Position&#39; &#39; Average&#39; &#39; HR&#39; &#39; RBI&#39; &#39; OPS&#39; &#39; JerseyNumber&#39;] [&#39;Manny Machado&#39; &#39; 3B&#39; &#39; .298&#39; &#39; 32&#39; &#39; 102&#39; &#39; .897&#39; &#39; 13&#39;] [&#39;Tatis Jr&#39; &#39; RF&#39; &#39; .281&#39; &#39; 42&#39; &#39; 97&#39; &#39; .975&#39; &#39; 23&#39;] [&#39;Juan Soto&#39; &#39; LF&#39; &#39; .242&#39; &#39; 27&#39; &#39; 62&#39; &#39; .853&#39; &#39; 22&#39;] [&#39;Xanger Bogaerts&#39; &#39; SS&#39; &#39; .307&#39; &#39; 15&#39; &#39; 73&#39; &#39; .833&#39; &#39; 2&#39;] [&#39;Nelson Cruz&#39; &#39; DH&#39; &#39; .234&#39; &#39; 10&#39; &#39; 64&#39; &#39; .651&#39; &#39; 32&#39;] [&#39;Matt Carpenter&#39; &#39; DH&#39; &#39; .305&#39; &#39; 15&#39; &#39; 37&#39; &#39; 1.138&#39; &#39; 14&#39;] [&#39;Cronezone&#39; &#39; 1B&#39; &#39; .239&#39; &#39; 17&#39; &#39; 88&#39; &#39; .722&#39; &#39; 9&#39;] [&#39;Ha-Seong Kim&#39; &#39; 2B&#39; &#39; .251&#39; &#39; 11&#39; &#39; 59&#39; &#39; .708&#39; &#39; 7&#39;] [&#39;Trent Grisham&#39; &#39; CF&#39; &#39; .184&#39; &#39; 17&#39; &#39; 53&#39; &#39; .626&#39; &#39; 1&#39;] [&#39;Luis Campusano&#39; &#39; C&#39; &#39; .250&#39; &#39; 1&#39; &#39; 5&#39; &#39; .593&#39; &#39; 12&#39;] [&#39;Austin Nola&#39; &#39; C&#39; &#39; .251&#39; &#39; 4&#39; &#39; 40&#39; &#39; .649&#39; &#39; 26&#39;] [&#39;Jose Azocar&#39; &#39; OF&#39; &#39; .257&#39; &#39; 0&#39; &#39; 10&#39; &#39; .630&#39; &#39; 28&#39;]] . padres = np.genfromtxt(&#39;files/padres.csv&#39;, delimiter=&#39;,&#39;, dtype=str, encoding=&#39;utf-8&#39;) # delimiter indicates that the data is separated into columns which is distinguished by commas # genfromtxt is used to read the csv file itself # dtype is used to have numpy automatically detect the data type in the csv file print(padres) . [[&#39;Name&#39; &#39; Position&#39; &#39; Average&#39; &#39; HR&#39; &#39; RBI&#39; &#39; OPS&#39; &#39; JerseyNumber&#39;] [&#39;Manny Machado&#39; &#39; 3B&#39; &#39; .298&#39; &#39; 32&#39; &#39; 102&#39; &#39; .897&#39; &#39; 13&#39;] [&#39;Tatis Jr&#39; &#39; RF&#39; &#39; .281&#39; &#39; 42&#39; &#39; 97&#39; &#39; .975&#39; &#39; 23&#39;] [&#39;Juan Soto&#39; &#39; LF&#39; &#39; .242&#39; &#39; 27&#39; &#39; 62&#39; &#39; .853&#39; &#39; 22&#39;] [&#39;Xanger Bogaerts&#39; &#39; SS&#39; &#39; .307&#39; &#39; 15&#39; &#39; 73&#39; &#39; .833&#39; &#39; 2&#39;] [&#39;Nelson Cruz&#39; &#39; DH&#39; &#39; .234&#39; &#39; 10&#39; &#39; 64&#39; &#39; .651&#39; &#39; 32&#39;] [&#39;Matt Carpenter&#39; &#39; DH&#39; &#39; .305&#39; &#39; 15&#39; &#39; 37&#39; &#39; 1.138&#39; &#39; 14&#39;] [&#39;Cronezone&#39; &#39; 1B&#39; &#39; .239&#39; &#39; 17&#39; &#39; 88&#39; &#39; .722&#39; &#39; 9&#39;] [&#39;Ha-Seong Kim&#39; &#39; 2B&#39; &#39; .251&#39; &#39; 11&#39; &#39; 59&#39; &#39; .708&#39; &#39; 7&#39;] [&#39;Trent Grisham&#39; &#39; CF&#39; &#39; .184&#39; &#39; 17&#39; &#39; 53&#39; &#39; .626&#39; &#39; 1&#39;] [&#39;Luis Campusano&#39; &#39; C&#39; &#39; .250&#39; &#39; 1&#39; &#39; 5&#39; &#39; .593&#39; &#39; 12&#39;] [&#39;Austin Nola&#39; &#39; C&#39; &#39; .251&#39; &#39; 4&#39; &#39; 40&#39; &#39; .649&#39; &#39; 26&#39;] [&#39;Jose Azocar&#39; &#39; OF&#39; &#39; .257&#39; &#39; 0&#39; &#39; 10&#39; &#39; .630&#39; &#39; 28&#39;]] .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/04/24/test.html",
            "relUrl": "/2023/04/24/test.html",
            "date": " • Apr 24, 2023"
        }
        
    
  
    
        ,"post9": {
            "title": "Tangible PANDAS",
            "content": ". How to implement PANDAS into DAILY LIFE . This related to pandas due to the fact that it displays the displaying of data allowing users to add and take away from it which shows the calling of procedures within frontend. . DATA1 | DATA2 | . DATA3 | DATA1 | . Click the button to remove the first row in the table. . Try it .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/04/18/PANDASSTART.html",
            "relUrl": "/2023/04/18/PANDASSTART.html",
            "date": " • Apr 18, 2023"
        }
        
    
  
    
        ,"post10": {
            "title": "NumPy Tangible Artifact",
            "content": "import numpy as np # define possible characters chars = np.array([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;j&#39;, &#39;k&#39;, &#39;l&#39;, &#39;m&#39;, &#39;n&#39;, &#39;o&#39;, &#39;p&#39;, &#39;q&#39;, &#39;r&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;v&#39;, &#39;w&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, &#39;G&#39;, &#39;H&#39;, &#39;I&#39;, &#39;J&#39;, &#39;K&#39;, &#39;L&#39;, &#39;M&#39;, &#39;N&#39;, &#39;O&#39;, &#39;P&#39;, &#39;Q&#39;, &#39;R&#39;, &#39;S&#39;, &#39;T&#39;, &#39;U&#39;, &#39;V&#39;, &#39;W&#39;, &#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;, &#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;]) # generate password password = &#39;&#39;.join(np.random.choice(chars, size=10)) print(password) . XjAaJ7Qak3 .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/04/18/NumpyTangibles.html",
            "relUrl": "/2023/04/18/NumpyTangibles.html",
            "date": " • Apr 18, 2023"
        }
        
    
  
    
        ,"post11": {
            "title": "Numpy Lesson Plan and Hacks",
            "content": "Lesson Plan . The lesson will cover various topics about Numpy and how to use it to perform mathematical functions and analyze data How to manipulate arrays and use mathematical expressions to solve | How to analyze data and read CSV files multiple ways | . | There will be multiple &quot;popcorn&quot; hacks within the lesson as well | . 1. Intro to NumPy and the features it consists . Numpy, by definition, is the fundamental package for scientific computing in Python which can be used to perform mathematical operations, providing multidimensional array objects, and makes data analysis much easier. Numpy is very important and useful when it comes to data analysis, as it can easily use its features to complete and perform any mathematical operation, as well as analyze data files. . If you don&#39;t already have numpy installed, you can do so using conda install numpy or pip install numpy . Once that is complete, to import numpy in your code, all you must do is: . import numpy as np . 2. Using NumPy to create arrays . An array is the central data structure of the NumPy library. They are used as containers which are able to store more than one item at the same time. Using the function np.array is used to create an array, in which you can create multidimensional arrays. . Shown below is how to create a 1D array: . a = np.array([1, 2, 3]) print(a) # this creates a 1D array . [1 2 3] . How could you create a 3D array based on knowing how to make a 1D array? . . Arrays can be printed in different ways, especially a more readable format. As we have seen, arrays are printed in rows and columns, but we can change that by using the reshape function . c = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) print(c.reshape(1, 9)) # organizes it all in a single line of output . [[1 2 3 4 5 6 7 8 9]] . In the code segment below, we can also specially select certain rows and columns from the array to further analyze selective data. . print(c[1:, :2]) # the 1: means &quot;start at row 1 and select all the remaining rows&quot; # the :2 means &quot;select the first two columns&quot; . [[4 5] [7 8]] . 3. Basic array operations . One of the most basic operations that can be performed on arrays is arithmetic operations. With numpy, it is very easy to perform arithmetic operations on arrays. You can add, subtract, multiply and divide arrays, just like you would with regular numbers. When performing these operations, numpy applies the operation element-wise, meaning that it performs the operation on each element in the array separately. This makes it easy to perform operations on large amounts of data quickly and efficiently. . import numpy as np a = np.array([1, 2, 3, 4, 5, 6]) a = a * 2 z = a/4 print(a) print(z) . [ 2 4 6 8 10 12] [0.5 1. 1.5 2. 2.5 3. ] . b = np.array([1, 2, 3]) c = np.array([4, 5, 6]) print(b + c) # adds each value based on the column the integer is in print(b - c) # subtracts each value based on the column the integer is in print(b * c) # multiplies each value based on the column the integer is in print(b / c) # divides each value based on the column the integer is in . [5 7 9] [-3 -3 -3] [ 4 10 18] [0.25 0.4 0.5 ] . d = np.exp(b) e = np.sqrt(b) print(d) print(e) . [ 2.71828183 7.3890561 20.08553692] [1. 1.41421356 1.73205081] . From the knowledge of how to use more advanced mathematical expressions than the basic 4 mathematical operations such as exponent and square root, now can you code how to calculate the 3 main trig expressions (sin, cos, tan), natural log, and log10 of a 1D array. . # calculate cos # calculate tan # calculate natural log # calculate log10 . 4. Data analysis using numpy . Numpy provides a convenient and powerful way to perform data analysis tasks on large datasets. One of the most common tasks in data analysis is finding the mean, median, and standard deviation of a dataset. Numpy provides functions to perform these operations quickly and easily. The mean function calculates the average value of the data, while the median function calculates the middle value in the data. The standard deviation function calculates how spread out the data is from the mean. Additionally, numpy provides functions to find the minimum and maximum values in the data. These functions are very useful for gaining insight into the properties of large datasets and can be used for a wide range of data analysis tasks. . data = np.array([2, 5, 12, 13, 19]) print(np.mean(data)) # finds the mean of the dataset print(np.median(data)) # finds the median of the dataset print(np.std(data)) # finds the standard deviation of the dataset print(np.min(data)) # finds the min of the dataset print(np.max(data)) # finds the max of the dataset . 10.2 12.0 6.04648658313239 2 19 . Now from learning this, can you find a different way from how we can solve the sum or products of a dataset other than how we learned before? . . Numpy also has the ability to handle CSV files, which are commonly used to store and exchange large datasets. By importing CSV files into numpy arrays, we can easily perform complex operations and analysis on the data, making numpy an essential tool for data scientists and researchers. . genfromtxt and loadtxt are two functions in the numpy library that can be used to read data from text files, including CSV files. . genfromtxt is a more advanced function that can be used to read text files that have more complex structures, including CSV files. genfromtxt can handle files that have missing or invalid data, or files that have columns of different data types. It can also be used to skip header lines or to read only specific columns from the file. . import numpy as np padres = np.genfromtxt(&#39;files/padres.csv&#39;, delimiter=&#39;,&#39;, dtype=str, encoding=&#39;utf-8&#39;) # delimiter indicates that the data is separated into columns which is distinguished by commas # genfromtxt is used to read the csv file itself # dtype is used to have numpy automatically detect the data type in the csv file print(padres) . [[&#39;Name&#39; &#39; Position&#39; &#39; Average&#39; &#39; HR&#39; &#39; RBI&#39; &#39; OPS&#39; &#39; JerseyNumber&#39;] [&#39;Manny Machado&#39; &#39; 3B&#39; &#39; .298&#39; &#39; 32&#39; &#39; 102&#39; &#39; .897&#39; &#39; 13&#39;] [&#39;Fernando Tatis Jr&#39; &#39; RF&#39; &#39; .281&#39; &#39; 42&#39; &#39; 97&#39; &#39; .975&#39; &#39; 23&#39;] [&#39;Juan Soto&#39; &#39; LF&#39; &#39; .242&#39; &#39; 27&#39; &#39; 62&#39; &#39; .853&#39; &#39; 22&#39;] [&#39;Xander Bogaerts&#39; &#39; SS&#39; &#39; .307&#39; &#39; 15&#39; &#39; 73&#39; &#39; .833&#39; &#39; 2&#39;] [&#39;Nelson Cruz&#39; &#39; DH&#39; &#39; .234&#39; &#39; 10&#39; &#39; 64&#39; &#39; .651&#39; &#39; 32&#39;] [&#39;Matt Carpenter&#39; &#39; DH&#39; &#39; .305&#39; &#39; 15&#39; &#39; 37&#39; &#39; 1.138&#39; &#39; 14&#39;] [&#39;Jake Cronenworth&#39; &#39; 1B&#39; &#39; .239&#39; &#39; 17&#39; &#39; 88&#39; &#39; .722&#39; &#39; 9&#39;] [&#39;Ha-Seong Kim&#39; &#39; 2B&#39; &#39; .251&#39; &#39; 11&#39; &#39; 59&#39; &#39; .708&#39; &#39; 7&#39;] [&#39;Trent Grisham&#39; &#39; CF&#39; &#39; .184&#39; &#39; 17&#39; &#39; 53&#39; &#39; .626&#39; &#39; 1&#39;] [&#39;Luis Campusano&#39; &#39; C&#39; &#39; .250&#39; &#39; 1&#39; &#39; 5&#39; &#39; .593&#39; &#39; 12&#39;] [&#39;Austin Nola&#39; &#39; C&#39; &#39; .251&#39; &#39; 4&#39; &#39; 40&#39; &#39; .649&#39; &#39; 26&#39;] [&#39;Jose Azocar&#39; &#39; OF&#39; &#39; .257&#39; &#39; 0&#39; &#39; 10&#39; &#39; .630&#39; &#39; 28&#39;]] . loadtxt is a simpler function that can be used to read simple text files that have a regular structure, such as files that have only one type of data (such as all integers or all floats). loadtxt can be faster than genfromtxt because it assumes that the data in the file is well-structured and can be easily parsed. . import numpy as np padres = np.loadtxt(&#39;files/padres.csv&#39;, delimiter=&#39;,&#39;, dtype=str, encoding=&#39;utf-8&#39;) print(padres) . [[&#39;Name&#39; &#39; Position&#39; &#39; Average&#39; &#39; HR&#39; &#39; RBI&#39; &#39; OPS&#39; &#39; JerseyNumber&#39;] [&#39;Manny Machado&#39; &#39; 3B&#39; &#39; .298&#39; &#39; 32&#39; &#39; 102&#39; &#39; .897&#39; &#39; 13&#39;] [&#39;Tatis Jr&#39; &#39; RF&#39; &#39; .281&#39; &#39; 42&#39; &#39; 97&#39; &#39; .975&#39; &#39; 23&#39;] [&#39;Juan Soto&#39; &#39; LF&#39; &#39; .242&#39; &#39; 27&#39; &#39; 62&#39; &#39; .853&#39; &#39; 22&#39;] [&#39;Xanger Bogaerts&#39; &#39; SS&#39; &#39; .307&#39; &#39; 15&#39; &#39; 73&#39; &#39; .833&#39; &#39; 2&#39;] [&#39;Nelson Cruz&#39; &#39; DH&#39; &#39; .234&#39; &#39; 10&#39; &#39; 64&#39; &#39; .651&#39; &#39; 32&#39;] [&#39;Matt Carpenter&#39; &#39; DH&#39; &#39; .305&#39; &#39; 15&#39; &#39; 37&#39; &#39; 1.138&#39; &#39; 14&#39;] [&#39;Cronezone&#39; &#39; 1B&#39; &#39; .239&#39; &#39; 17&#39; &#39; 88&#39; &#39; .722&#39; &#39; 9&#39;] [&#39;Ha-Seong Kim&#39; &#39; 2B&#39; &#39; .251&#39; &#39; 11&#39; &#39; 59&#39; &#39; .708&#39; &#39; 7&#39;] [&#39;Trent Grisham&#39; &#39; CF&#39; &#39; .184&#39; &#39; 17&#39; &#39; 53&#39; &#39; .626&#39; &#39; 1&#39;] [&#39;Luis Campusano&#39; &#39; C&#39; &#39; .250&#39; &#39; 1&#39; &#39; 5&#39; &#39; .593&#39; &#39; 12&#39;] [&#39;Austin Nola&#39; &#39; C&#39; &#39; .251&#39; &#39; 4&#39; &#39; 40&#39; &#39; .649&#39; &#39; 26&#39;] [&#39;Jose Azocar&#39; &#39; OF&#39; &#39; .257&#39; &#39; 0&#39; &#39; 10&#39; &#39; .630&#39; &#39; 28&#39;]] . for i in padres: print(&quot;,&quot;.join(i)) . Name, Position, Average, HR, RBI, OPS, JerseyNumber Manny Machado, Third Base, .298, 32, 102, .897, 13 Fernando Tatis Jr, Right Field, .281, 42, 97, .975, 23 Juan Soto, Left Field, .242, 27, 62, .853, 22 Xanger Bogaerts, Shortstop, .307, 15, 73, .833, 2 Nelson Cruz, Designated Hitter, .234, 10, 64, .651, 32 Matt Carpenter, Designated Hitter, .305, 15, 37, 1.138, 14 Jake Cronenworth, First Base, .239, 17, 88, .722, 9 Ha-Seong Kim, Second Base, .251, 11, 59, .708, 7 Trent Grisham, Center Field, .184, 17, 53, .626, 1 Luis Campusano, Catcher, .250, 1, 5, .593, 12 Austin Nola, Catcher, .251, 4, 40, .649, 26 Jose Azocar, Outfield, .257, 0, 10, .630, 28 .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/04/16/Numpy-plan.html",
            "relUrl": "/2023/04/16/Numpy-plan.html",
            "date": " • Apr 16, 2023"
        }
        
    
  
    
        ,"post12": {
            "title": "Unit 2.4b Using Programs with Data, SQL",
            "content": "Database Programming is Program with Data . Each Tri 2 Final Project should be an example of a Program with Data. . Prepare to use SQLite in common Imperative Technique . Explore SQLite Connect object to establish database connection- Explore SQLite Cursor Object to fetch data from a table within a database | . Schema of Users table in Sqlite.db . Uses PRAGMA statement to read schema. . Describe Schema, here is resource Resource- What is a database schema? . What is the purpose of identity Column in SQL database? | What is the purpose of a primary key in SQL database? | What are the Data Types in SQL table? | . import sqlite3 database = &#39;instance/sqlite.db&#39; # this is location of database def schema(): # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL queries cursor = conn.cursor() # Fetch results of Schema results = cursor.execute(&quot;PRAGMA table_info(&#39;users&#39;)&quot;).fetchall() # Print the results for row in results: print(row) # Close the database connection conn.close() schema() . Reading Users table in Sqlite.db . Uses SQL SELECT statement to read data . What is a connection object? After you google it, what do you think it does? | Same for cursor object? | Look at conn object and cursor object in VSCode debugger. What attributes are in the object? | Is &quot;results&quot; an object? How do you know? | . import sqlite3 def read(): # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL queries cursor = conn.cursor() # Execute a SELECT statement to retrieve data from a table results = cursor.execute(&#39;SELECT * FROM users&#39;).fetchall() # Print the results if len(results) == 0: print(&quot;Table is empty&quot;) else: for row in results: print(row) # Close the cursor and connection objects cursor.close() conn.close() read() . Create a new User in table in Sqlite.db . Uses SQL INSERT to add row . Compore create() in both SQL lessons. What is better or worse in the two implementations? | Explain purpose of SQL INSERT. Is this the same as User init? | . import sqlite3 def create(): name = input(&quot;Enter your name:&quot;) uid = input(&quot;Enter your user id:&quot;) password = input(&quot;Enter your password&quot;) dob = input(&quot;Enter your date of birth &#39;YYYY-MM-DD&#39;&quot;) # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: # Execute an SQL command to insert data into a table cursor.execute(&quot;INSERT INTO users (_name, _uid, _password, _dob) VALUES (?, ?, ?, ?)&quot;, (name, uid, password, dob)) # Commit the changes to the database conn.commit() print(f&quot;A new user record {uid} has been created&quot;) except sqlite3.Error as error: print(&quot;Error while executing the INSERT:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #create() . Updating a User in table in Sqlite.db . Uses SQL UPDATE to modify password . What does the hacked part do? | Explain try/except, when would except occur? | What code seems to be repeated in each of these examples to point, why is it repeated? | . import sqlite3 def update(): uid = input(&quot;Enter user id to update&quot;) password = input(&quot;Enter updated password&quot;) if len(password) &lt; 2: message = &quot;hacked&quot; password = &#39;gothackednewpassword123&#39; else: message = &quot;successfully updated&quot; # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: # Execute an SQL command to update data in a table cursor.execute(&quot;UPDATE users SET _password = ? WHERE _uid = ?&quot;, (password, uid)) if cursor.rowcount == 0: # The uid was not found in the table print(f&quot;No uid {uid} was not found in the table&quot;) else: print(f&quot;The row with user id {uid} the password has been {message}&quot;) conn.commit() except sqlite3.Error as error: print(&quot;Error while executing the UPDATE:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #update() . Delete a User in table in Sqlite.db . Uses a delete function to remove a user based on a user input of the id. . Is DELETE a dangerous operation? Why? | In the print statemements, what is the &quot;f&quot; and what does {uid} do? | . import sqlite3 def delete(): uid = input(&quot;Enter user id to delete&quot;) # Connect to the database file conn = sqlite3.connect(database) # Create a cursor object to execute SQL commands cursor = conn.cursor() try: cursor.execute(&quot;DELETE FROM users WHERE _uid = ?&quot;, (uid,)) if cursor.rowcount == 0: # The uid was not found in the table print(f&quot;No uid {uid} was not found in the table&quot;) else: # The uid was found in the table and the row was deleted print(f&quot;The row with uid {uid} was successfully deleted&quot;) conn.commit() except sqlite3.Error as error: print(&quot;Error while executing the DELETE:&quot;, error) # Close the cursor and connection objects cursor.close() conn.close() #delete() . Menu Interface to CRUD operations . CRUD and Schema interactions from one location by running menu. Observe input at the top of VSCode, observe output underneath code cell. . Why does the menu repeat? | Could you refactor this menu? Make it work with a List? | . def menu(): operation = input(&quot;Enter: (C)reate (R)ead (U)pdate or (D)elete or (S)chema&quot;) if operation.lower() == &#39;c&#39;: create() elif operation.lower() == &#39;r&#39;: read() elif operation.lower() == &#39;u&#39;: update() elif operation.lower() == &#39;d&#39;: delete() elif operation.lower() == &#39;s&#39;: schema() elif len(operation)==0: # Escape Key return else: print(&quot;Please enter c, r, u, or d&quot;) menu() # recursion, repeat menu try: menu() # start menu except: print(&quot;Perform Jupyter &#39;Run All&#39; prior to starting menu&quot;) . Hacks . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In this implementation, do you see procedural abstraction? | In 2.4a or 2.4b lecture Do you see data abstraction? Complement this with Debugging example. | Use Imperative or OOP style to Create a new Table or do something that applies to your CPT project. | . | . Reference... sqlite documentation .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/03/16/AP-unit2-4b.html",
            "relUrl": "/2023/03/16/AP-unit2-4b.html",
            "date": " • Mar 16, 2023"
        }
        
    
  
    
        ,"post13": {
            "title": "Unit 2.4a Using Programs with Data, SQLAlchemy",
            "content": "Database and SQLAlchemy . In this blog we will explore using programs with data, focused on Databases. We will use SQLite Database to learn more about using Programs with Data. Use Debugging through these examples to examine Objects created in Code. . College Board talks about ideas like . Program Usage. &quot;iterative and interactive way when processing information&quot; | Managing Data. &quot;classifying data are part of the process in using programs&quot;, &quot;data files in a Table&quot; | Insight &quot;insight and knowledge can be obtained from ... digitally represented information&quot; | Filter systems. &#39;tools for finding information and recognizing patterns&quot; | Application. &quot;the preserve has two databases&quot;, &quot;an employee wants to count the number of book&quot; | . | PBL, Databases, Iterative/OOP . Iterative. Refers to a sequence of instructions or code being repeated until a specific end result is achieved | OOP. A computer programming model that organizes software design around data, or objects, rather than functions and logic | SQL. Structured Query Language, abbreviated as SQL, is a language used in programming, managing, and structuring data | . | . Imports and Flask Objects . Defines and key object creations . Comment on where you have observed these working? Provide a defintion of purpose. Flask app object | SQLAlchemy db object | | . &quot;&quot;&quot; These imports define the key objects &quot;&quot;&quot; from flask import Flask from flask_sqlalchemy import SQLAlchemy &quot;&quot;&quot; These object and definitions are used throughout the Jupyter Notebook. &quot;&quot;&quot; # Setup of key Flask object (app) app = Flask(__name__) # Setup SQLAlchemy object and properties for the database (db) database = &#39;sqlite:///sqlite.db&#39; # path and filename of database app.config[&#39;SQLALCHEMY_TRACK_MODIFICATIONS&#39;] = False app.config[&#39;SQLALCHEMY_DATABASE_URI&#39;] = database app.config[&#39;SECRET_KEY&#39;] = &#39;SECRET_KEY&#39; db = SQLAlchemy() # This belongs in place where it runs once per project db.init_app(app) . Model Definition . Define columns, initialization, and CRUD methods for users table in sqlite.db . Comment on these items in the class, purpose and defintion. class User | db.Model inheritance | init method | @property, @&lt;column&gt;.setter | create, read, update, delete methods | . | . &quot;&quot;&quot; database dependencies to support sqlite examples &quot;&quot;&quot; import datetime from datetime import datetime import json from sqlalchemy.exc import IntegrityError from werkzeug.security import generate_password_hash, check_password_hash &#39;&#39;&#39; Tutorial: https://www.sqlalchemy.org/library.html#tutorials, try to get into a Python shell and follow along &#39;&#39;&#39; # Define the User class to manage actions in the &#39;users&#39; table # -- Object Relational Mapping (ORM) is the key concept of SQLAlchemy # -- a.) db.Model is like an inner layer of the onion in ORM # -- b.) User represents data we want to store, something that is built on db.Model # -- c.) SQLAlchemy ORM is layer on top of SQLAlchemy Core, then SQLAlchemy engine, SQL class User(db.Model): __tablename__ = &#39;users&#39; # table name is plural, class name is singular # Define the User schema with &quot;vars&quot; from object id = db.Column(db.Integer, primary_key=True) _name = db.Column(db.String(255), unique=False, nullable=False) _uid = db.Column(db.String(255), unique=True, nullable=False) _password = db.Column(db.String(255), unique=False, nullable=False) _dob = db.Column(db.Date) # constructor of a User object, initializes the instance variables within object (self) def __init__(self, name, uid, password=&quot;123qwerty&quot;, dob=datetime.today()): self._name = name # variables with self prefix become part of the object, self._uid = uid self.set_password(password) if isinstance(dob, str): # not a date type dob = date=datetime.today() self._dob = dob # a name getter method, extracts name from object @property def name(self): return self._name # a setter function, allows name to be updated after initial object creation @name.setter def name(self, name): self._name = name # a getter method, extracts uid from object @property def uid(self): return self._uid # a setter function, allows uid to be updated after initial object creation @uid.setter def uid(self, uid): self._uid = uid # check if uid parameter matches user id in object, return boolean def is_uid(self, uid): return self._uid == uid @property def password(self): return self._password[0:10] + &quot;...&quot; # because of security only show 1st characters # update password, this is conventional method used for setter def set_password(self, password): &quot;&quot;&quot;Create a hashed password.&quot;&quot;&quot; self._password = generate_password_hash(password, method=&#39;sha256&#39;) # check password parameter against stored/encrypted password def is_password(self, password): &quot;&quot;&quot;Check against hashed password.&quot;&quot;&quot; result = check_password_hash(self._password, password) return result # dob property is returned as string, a string represents date outside object @property def dob(self): dob_string = self._dob.strftime(&#39;%m-%d-%Y&#39;) return dob_string # dob setter, verifies date type before it is set or default to today @dob.setter def dob(self, dob): if isinstance(dob, str): # not a date type dob = date=datetime.today() self._dob = dob # age is calculated field, age is returned according to date of birth @property def age(self): today = datetime.today() return today.year - self._dob.year - ((today.month, today.day) &lt; (self._dob.month, self._dob.day)) # output content using str(object) is in human readable form # output content using json dumps, this is ready for API response def __str__(self): return json.dumps(self.read()) # CRUD create/add a new record to the table # returns self or None on error def create(self): try: # creates a person object from User(db.Model) class, passes initializers db.session.add(self) # add prepares to persist person object to Users table db.session.commit() # SqlAlchemy &quot;unit of work pattern&quot; requires a manual commit return self except IntegrityError: db.session.remove() return None # CRUD read converts self to dictionary # returns dictionary def read(self): return { &quot;id&quot;: self.id, &quot;name&quot;: self.name, &quot;uid&quot;: self.uid, &quot;dob&quot;: self.dob, &quot;age&quot;: self.age, } # CRUD update: updates user name, password, phone # returns self def update(self, name=&quot;&quot;, uid=&quot;&quot;, password=&quot;&quot;): &quot;&quot;&quot;only updates values with length&quot;&quot;&quot; if len(name) &gt; 0: self.name = name if len(uid) &gt; 0: self.uid = uid if len(password) &gt; 0: self.set_password(password) db.session.add(self) # performs update when id exists db.session.commit() return self # CRUD delete: remove self # None def delete(self): db.session.delete(self) db.session.commit() return None . Initial Data . Uses SQLALchemy db.create_all() to initialize rows into sqlite.db . Comment on how these work? Create All Tables from db Object | User Object Constructors | Try / Except | | . &quot;&quot;&quot;Database Creation and Testing &quot;&quot;&quot; # Builds working data for testing def initUsers(): with app.app_context(): &quot;&quot;&quot;Create database and tables&quot;&quot;&quot; db.create_all() &quot;&quot;&quot;Tester data for table&quot;&quot;&quot; u1 = User(name=&#39;Thomas Edison&#39;, uid=&#39;toby&#39;, password=&#39;123toby&#39;, dob=datetime(1847, 2, 11)) u2 = User(name=&#39;Nikola Tesla&#39;, uid=&#39;niko&#39;, password=&#39;123niko&#39;) u3 = User(name=&#39;Alexander Graham Bell&#39;, uid=&#39;lex&#39;, password=&#39;123lex&#39;) u4 = User(name=&#39;Eli Whitney&#39;, uid=&#39;whit&#39;, password=&#39;123whit&#39;) u5 = User(name=&#39;Indiana Jones&#39;, uid=&#39;indi&#39;, dob=datetime(1920, 10, 21)) u6 = User(name=&#39;Marion Ravenwood&#39;, uid=&#39;raven&#39;, dob=datetime(1921, 10, 21)) users = [u1, u2, u3, u4, u5, u6] &quot;&quot;&quot;Builds sample user/note(s) data&quot;&quot;&quot; for user in users: try: &#39;&#39;&#39;add user to table&#39;&#39;&#39; object = user.create() print(f&quot;Created new uid {object.uid}&quot;) except: # error raised if object nit created &#39;&#39;&#39;fails with bad or duplicate data&#39;&#39;&#39; print(f&quot;Records exist uid {user.uid}, or error.&quot;) initUsers() . OperationalError Traceback (most recent call last) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3256, in Engine._wrap_pool_connect(self, fn, connection) 3255 try: -&gt; 3256 return fn() 3257 except dialect.dbapi.Error as e: File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:310, in Pool.connect(self) 303 &#34;&#34;&#34;Return a DBAPI connection from the pool. 304 305 The connection is instrumented such that when its (...) 308 309 &#34;&#34;&#34; --&gt; 310 return _ConnectionFairy._checkout(self) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:868, in _ConnectionFairy._checkout(cls, pool, threadconns, fairy) 867 if not fairy: --&gt; 868 fairy = _ConnectionRecord.checkout(pool) 870 fairy._pool = pool File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:476, in _ConnectionRecord.checkout(cls, pool) 474 @classmethod 475 def checkout(cls, pool): --&gt; 476 rec = pool._do_get() 477 try: File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/impl.py:256, in NullPool._do_get(self) 255 def _do_get(self): --&gt; 256 return self._create_connection() File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:256, in Pool._create_connection(self) 254 &#34;&#34;&#34;Called by subclasses to create a new ConnectionRecord.&#34;&#34;&#34; --&gt; 256 return _ConnectionRecord(self) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:371, in _ConnectionRecord.__init__(self, pool, connect) 370 if connect: --&gt; 371 self.__connect() 372 self.finalize_callback = deque() File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:666, in _ConnectionRecord.__connect(self) 665 with util.safe_reraise(): --&gt; 666 pool.logger.debug(&#34;Error on connect(): %s&#34;, e) 667 else: 668 # in SQLAlchemy 1.4 the first_connect event is not used by 669 # the engine, so this will usually not be set File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py:70, in safe_reraise.__exit__(self, type_, value, traceback) 69 if not self.warn_only: &gt; 70 compat.raise_( 71 exc_value, 72 with_traceback=exc_tb, 73 ) 74 else: File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/util/compat.py:207, in raise_(***failed resolving arguments***) 206 try: --&gt; 207 raise exception 208 finally: 209 # credit to 210 # https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/ 211 # as the __traceback__ object creates a cycle File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:661, in _ConnectionRecord.__connect(self) 660 self.starttime = time.time() --&gt; 661 self.dbapi_connection = connection = pool._invoke_creator(self) 662 pool.logger.debug(&#34;Created new connection %r&#34;, connection) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/create.py:590, in create_engine.&lt;locals&gt;.connect(connection_record) 589 return connection --&gt; 590 return dialect.connect(*cargs, **cparams) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/default.py:597, in DefaultDialect.connect(self, *cargs, **cparams) 595 def connect(self, *cargs, **cparams): 596 # inherits the docstring from interfaces.Dialect.connect --&gt; 597 return self.dbapi.connect(*cargs, **cparams) OperationalError: unable to open database file The above exception was the direct cause of the following exception: OperationalError Traceback (most recent call last) /home/user/vscode/APCSP/_notebooks/2023-03-13-AP-unit2-4a.ipynb Cell 8 in &lt;cell line: 30&gt;() &lt;a href=&#39;vscode-notebook-cell://wsl%2Bubuntu-20.04/home/user/vscode/APCSP/_notebooks/2023-03-13-AP-unit2-4a.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26&#39;&gt;27&lt;/a&gt; &#39;&#39;&#39;fails with bad or duplicate data&#39;&#39;&#39; &lt;a href=&#39;vscode-notebook-cell://wsl%2Bubuntu-20.04/home/user/vscode/APCSP/_notebooks/2023-03-13-AP-unit2-4a.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27&#39;&gt;28&lt;/a&gt; print(f&#34;Records exist uid {user.uid}, or error.&#34;) &gt; &lt;a href=&#39;vscode-notebook-cell://wsl%2Bubuntu-20.04/home/user/vscode/APCSP/_notebooks/2023-03-13-AP-unit2-4a.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29&#39;&gt;30&lt;/a&gt; initUsers() /home/user/vscode/APCSP/_notebooks/2023-03-13-AP-unit2-4a.ipynb Cell 8 in initUsers() &lt;a href=&#39;vscode-notebook-cell://wsl%2Bubuntu-20.04/home/user/vscode/APCSP/_notebooks/2023-03-13-AP-unit2-4a.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5&#39;&gt;6&lt;/a&gt; with app.app_context(): &lt;a href=&#39;vscode-notebook-cell://wsl%2Bubuntu-20.04/home/user/vscode/APCSP/_notebooks/2023-03-13-AP-unit2-4a.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6&#39;&gt;7&lt;/a&gt; &#34;&#34;&#34;Create database and tables&#34;&#34;&#34; -&gt; &lt;a href=&#39;vscode-notebook-cell://wsl%2Bubuntu-20.04/home/user/vscode/APCSP/_notebooks/2023-03-13-AP-unit2-4a.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7&#39;&gt;8&lt;/a&gt; db.create_all() &lt;a href=&#39;vscode-notebook-cell://wsl%2Bubuntu-20.04/home/user/vscode/APCSP/_notebooks/2023-03-13-AP-unit2-4a.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8&#39;&gt;9&lt;/a&gt; &#34;&#34;&#34;Tester data for table&#34;&#34;&#34; &lt;a href=&#39;vscode-notebook-cell://wsl%2Bubuntu-20.04/home/user/vscode/APCSP/_notebooks/2023-03-13-AP-unit2-4a.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9&#39;&gt;10&lt;/a&gt; u1 = User(name=&#39;Thomas Edison&#39;, uid=&#39;toby&#39;, password=&#39;123toby&#39;, dob=datetime(1847, 2, 11)) File ~/anaconda3/lib/python3.9/site-packages/flask_sqlalchemy/extension.py:868, in SQLAlchemy.create_all(self, bind_key) 851 def create_all(self, bind_key: str | None | list[str | None] = &#34;__all__&#34;) -&gt; None: 852 &#34;&#34;&#34;Create tables that do not exist in the database by calling 853 ``metadata.create_all()`` for all or some bind keys. This does not 854 update existing tables, use a migration library for that. (...) 866 Added the ``bind`` and ``app`` parameters. 867 &#34;&#34;&#34; --&gt; 868 self._call_for_binds(bind_key, &#34;create_all&#34;) File ~/anaconda3/lib/python3.9/site-packages/flask_sqlalchemy/extension.py:849, in SQLAlchemy._call_for_binds(self, bind_key, op_name) 846 raise sa.exc.UnboundExecutionError(message) from None 848 metadata = self.metadatas[key] --&gt; 849 getattr(metadata, op_name)(bind=engine) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/sql/schema.py:4864, in MetaData.create_all(self, bind, tables, checkfirst) 4862 if bind is None: 4863 bind = _bind_or_error(self) -&gt; 4864 bind._run_ddl_visitor( 4865 ddl.SchemaGenerator, self, checkfirst=checkfirst, tables=tables 4866 ) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3122, in Engine._run_ddl_visitor(self, visitorcallable, element, **kwargs) 3121 def _run_ddl_visitor(self, visitorcallable, element, **kwargs): -&gt; 3122 with self.begin() as conn: 3123 conn._run_ddl_visitor(visitorcallable, element, **kwargs) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3038, in Engine.begin(self, close_with_result) 3036 conn = self.connect() 3037 else: -&gt; 3038 conn = self.connect(close_with_result=close_with_result) 3039 try: 3040 trans = conn.begin() File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3210, in Engine.connect(self, close_with_result) 3195 def connect(self, close_with_result=False): 3196 &#34;&#34;&#34;Return a new :class:`_engine.Connection` object. 3197 3198 The :class:`_engine.Connection` object is a facade that uses a DBAPI (...) 3207 3208 &#34;&#34;&#34; -&gt; 3210 return self._connection_cls(self, close_with_result=close_with_result) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:96, in Connection.__init__(self, engine, connection, close_with_result, _branch_from, _execution_options, _dispatch, _has_events, _allow_revalidate) 91 self._has_events = _branch_from._has_events 92 else: 93 self._dbapi_connection = ( 94 connection 95 if connection is not None &gt; 96 else engine.raw_connection() 97 ) 99 self._transaction = self._nested_transaction = None 100 self.__savepoint_seq = 0 File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3289, in Engine.raw_connection(self, _connection) 3267 def raw_connection(self, _connection=None): 3268 &#34;&#34;&#34;Return a &#34;raw&#34; DBAPI connection from the connection pool. 3269 3270 The returned object is a proxied version of the DBAPI (...) 3287 3288 &#34;&#34;&#34; -&gt; 3289 return self._wrap_pool_connect(self.pool.connect, _connection) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3259, in Engine._wrap_pool_connect(self, fn, connection) 3257 except dialect.dbapi.Error as e: 3258 if connection is None: -&gt; 3259 Connection._handle_dbapi_exception_noconnection( 3260 e, dialect, self 3261 ) 3262 else: 3263 util.raise_( 3264 sys.exc_info()[1], with_traceback=sys.exc_info()[2] 3265 ) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:2106, in Connection._handle_dbapi_exception_noconnection(cls, e, dialect, engine) 2104 util.raise_(newraise, with_traceback=exc_info[2], from_=e) 2105 elif should_wrap: -&gt; 2106 util.raise_( 2107 sqlalchemy_exception, with_traceback=exc_info[2], from_=e 2108 ) 2109 else: 2110 util.raise_(exc_info[1], with_traceback=exc_info[2]) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/util/compat.py:207, in raise_(***failed resolving arguments***) 204 exception.__cause__ = replace_context 206 try: --&gt; 207 raise exception 208 finally: 209 # credit to 210 # https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/ 211 # as the __traceback__ object creates a cycle 212 del exception, replace_context, from_, with_traceback File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/base.py:3256, in Engine._wrap_pool_connect(self, fn, connection) 3254 dialect = self.dialect 3255 try: -&gt; 3256 return fn() 3257 except dialect.dbapi.Error as e: 3258 if connection is None: File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:310, in Pool.connect(self) 302 def connect(self): 303 &#34;&#34;&#34;Return a DBAPI connection from the pool. 304 305 The connection is instrumented such that when its (...) 308 309 &#34;&#34;&#34; --&gt; 310 return _ConnectionFairy._checkout(self) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:868, in _ConnectionFairy._checkout(cls, pool, threadconns, fairy) 865 @classmethod 866 def _checkout(cls, pool, threadconns=None, fairy=None): 867 if not fairy: --&gt; 868 fairy = _ConnectionRecord.checkout(pool) 870 fairy._pool = pool 871 fairy._counter = 0 File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:476, in _ConnectionRecord.checkout(cls, pool) 474 @classmethod 475 def checkout(cls, pool): --&gt; 476 rec = pool._do_get() 477 try: 478 dbapi_connection = rec.get_connection() File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/impl.py:256, in NullPool._do_get(self) 255 def _do_get(self): --&gt; 256 return self._create_connection() File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:256, in Pool._create_connection(self) 253 def _create_connection(self): 254 &#34;&#34;&#34;Called by subclasses to create a new ConnectionRecord.&#34;&#34;&#34; --&gt; 256 return _ConnectionRecord(self) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:371, in _ConnectionRecord.__init__(self, pool, connect) 369 self.__pool = pool 370 if connect: --&gt; 371 self.__connect() 372 self.finalize_callback = deque() File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:666, in _ConnectionRecord.__connect(self) 664 except Exception as e: 665 with util.safe_reraise(): --&gt; 666 pool.logger.debug(&#34;Error on connect(): %s&#34;, e) 667 else: 668 # in SQLAlchemy 1.4 the first_connect event is not used by 669 # the engine, so this will usually not be set 670 if pool.dispatch.first_connect: File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py:70, in safe_reraise.__exit__(self, type_, value, traceback) 68 self._exc_info = None # remove potential circular references 69 if not self.warn_only: &gt; 70 compat.raise_( 71 exc_value, 72 with_traceback=exc_tb, 73 ) 74 else: 75 if not compat.py3k and self._exc_info and self._exc_info[1]: 76 # emulate Py3K&#39;s behavior of telling us when an exception 77 # occurs in an exception handler. File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/util/compat.py:207, in raise_(***failed resolving arguments***) 204 exception.__cause__ = replace_context 206 try: --&gt; 207 raise exception 208 finally: 209 # credit to 210 # https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/ 211 # as the __traceback__ object creates a cycle 212 del exception, replace_context, from_, with_traceback File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/pool/base.py:661, in _ConnectionRecord.__connect(self) 659 try: 660 self.starttime = time.time() --&gt; 661 self.dbapi_connection = connection = pool._invoke_creator(self) 662 pool.logger.debug(&#34;Created new connection %r&#34;, connection) 663 self.fresh = True File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/create.py:590, in create_engine.&lt;locals&gt;.connect(connection_record) 588 if connection is not None: 589 return connection --&gt; 590 return dialect.connect(*cargs, **cparams) File ~/anaconda3/lib/python3.9/site-packages/sqlalchemy/engine/default.py:597, in DefaultDialect.connect(self, *cargs, **cparams) 595 def connect(self, *cargs, **cparams): 596 # inherits the docstring from interfaces.Dialect.connect --&gt; 597 return self.dbapi.connect(*cargs, **cparams) OperationalError: (sqlite3.OperationalError) unable to open database file (Background on this error at: https://sqlalche.me/e/14/e3q8) . Check for given Credentials in users table in sqlite.db . Use of ORM Query object and custom methods to identify user to credentials uid and password . Comment on purpose of following User.query.filter_by | user.password | | . def find_by_uid(uid): with app.app_context(): user = User.query.filter_by(_uid=uid).first() return user # returns user object # Check credentials by finding user and verify password def check_credentials(uid, password): # query email and return user record user = find_by_uid(uid) if user == None: return False if (user.is_password(password)): return True return False #check_credentials(&quot;indi&quot;, &quot;123qwerty&quot;) . Create a new User in table in Sqlite.db . Uses SQLALchemy and custom user.create() method to add row. . Comment on purpose of following user.find_by_uid() and try/except | user = User(...) | user.dob and try/except | user.create() and try/except | | . def create(): # optimize user time to see if uid exists uid = input(&quot;Enter your user id:&quot;) user = find_by_uid(uid) try: print(&quot;Found n&quot;, user.read()) return except: pass # keep going # request value that ensure creating valid object name = input(&quot;Enter your name:&quot;) password = input(&quot;Enter your password&quot;) # Initialize User object before date user = User(name=name, uid=uid, password=password ) # create user.dob, fail with today as dob dob = input(&quot;Enter your date of birth &#39;YYYY-MM-DD&#39;&quot;) try: user.dob = datetime.strptime(dob, &#39;%Y-%m-%d&#39;).date() except ValueError: user.dob = datetime.today() print(f&quot;Invalid date {dob} require YYYY-mm-dd, date defaulted to {user.dob}&quot;) # write object to database with app.app_context(): try: object = user.create() print(&quot;Created n&quot;, object.read()) except: # error raised if object not created print(&quot;Unknown error uid {uid}&quot;) create() . Reading users table in sqlite.db . Uses SQLALchemy query.all method to read data . Comment on purpose of following User.query.all | json_ready assignment, google List Comprehension | | . # SQLAlchemy extracts all users from database, turns each user into JSON def read(): with app.app_context(): table = User.query.all() json_ready = [user.read() for user in table] # &quot;List Comprehensions&quot;, for each user add user.read() to list return json_ready read() . Hacks . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | Change blog to your own database. | Add additional CRUD Add Update functionality to this blog. | Add Delete functionality to this blog. | . | .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/03/13/AP-unit2-4a.html",
            "relUrl": "/2023/03/13/AP-unit2-4a.html",
            "date": " • Mar 13, 2023"
        }
        
    
  
    
        ,"post14": {
            "title": "Unit 2.3 Extracting Information from Data, Pandas",
            "content": "Files To Get . Save this file to your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/2023-03-09-AP-unit2-3.ipynb . Save these files into a subfolder named files in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/data.csv . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/grade.json . Save this image into a subfolder named images in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/images/table_dataframe.png . Pandas and DataFrames . In this lesson we will be exploring data analysis using Pandas. . College Board talks about ideas like Tools. &quot;the ability to process data depends on users capabilities and their tools&quot; | Combining Data. &quot;combine county data sets&quot; | Status on Data&quot;determining the artist with the greatest attendance during a particular month&quot; | Data poses challenge. &quot;the need to clean data&quot;, &quot;incomplete data&quot; | . | . From Pandas Overview -- When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you to explore, clean, and process your data. In pandas, a data table is called a DataFrame. | . . &#39;&#39;&#39;Pandas is used to gather data sets through its DataFrames implementation&#39;&#39;&#39; import pandas as pd . Cleaning Data . When looking at a data set, check to see what data needs to be cleaned. Examples include: . Missing Data Points | Invalid Data | Inaccurate Data | . Run the following code to see what needs to be cleaned . df = pd.read_json(&#39;files/grade.json&#39;) print(df) # What part of the data set needs to be cleaned? # From PBL learning, what is a good time to clean data? Hint, remember Garbage in, Garbage out? . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 2 578 12 2.78 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 6 145 12 2.95 7 167 10 3.90 8 235 9th Grade 3.15 9 nil 9 2.80 10 469 11 3.45 11 456 10 2.75 . Extracting Info . Take a look at some features that the Pandas library has that extracts info from the dataset . DataFrame Extract Column . print(df[[&#39;GPA&#39;]]) print() #try two columns and remove the index from print statement print(df[[&#39;Student ID&#39;,&#39;GPA&#39;]].to_string(index=False)) . GPA 0 3.57 1 4.00 2 2.78 3 3.45 4 4.75 5 3.33 6 2.95 7 3.90 8 3.15 9 2.80 10 3.45 11 2.75 Student ID GPA 123 3.57 246 4.00 578 2.78 469 3.45 324 4.75 313 3.33 145 2.95 167 3.90 235 3.15 nil 2.80 469 3.45 456 2.75 . DataFrame Sort . print(df.sort_values(by=[&#39;GPA&#39;])) print() #sort the values in reverse order print(df.sort_values(by=[&#39;GPA&#39;], ascending=False)) . Student ID Year in School GPA 11 456 10 2.75 2 578 12 2.78 9 nil 9 2.80 6 145 12 2.95 8 235 9th Grade 3.15 5 313 20 3.33 3 469 11 3.45 10 469 11 3.45 0 123 12 3.57 7 167 10 3.90 1 246 10 4.00 4 324 Junior 4.75 Student ID Year in School GPA 4 324 Junior 4.75 1 246 10 4.00 7 167 10 3.90 0 123 12 3.57 3 469 11 3.45 10 469 11 3.45 5 313 20 3.33 8 235 9th Grade 3.15 6 145 12 2.95 9 nil 9 2.80 2 578 12 2.78 11 456 10 2.75 . DataFrame Selection or Filter . print(df[df.GPA &gt; 3.00]) . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 7 167 10 3.90 8 235 9th Grade 3.15 10 469 11 3.45 . DataFrame Selection Max and Min . print(df[df.GPA == df.GPA.max()]) print() print(df[df.GPA == df.GPA.min()]) . Student ID Year in School GPA 4 324 Junior 4.75 Student ID Year in School GPA 11 456 10 2.75 . Create your own DataFrame . Using Pandas allows you to create your own DataFrame in Python. . Python Dictionary to Pandas DataFrame . import pandas as pd #the data can be stored as a python dictionary dict = { &quot;calories&quot;: [420, 380, 390], &quot;duration&quot;: [50, 40, 45] } #stores the data in a data frame print(&quot;-Dict_to_DF&quot;) df = pd.DataFrame(dict) print(df) print(&quot;-Dict_to_DF_labels--&quot;) #or with the index argument, you can label rows. df = pd.DataFrame(dict, index = [&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;]) print(df) . -Dict_to_DF calories duration 0 420 50 1 380 40 2 390 45 -Dict_to_DF_labels-- calories duration day1 420 50 day2 380 40 day3 390 45 . Examine DataFrame Rows . print(&quot;-Examine Selected Rows&quot;) #use a list for multiple labels: print(df.loc[[&quot;day1&quot;, &quot;day3&quot;]]) #refer to the row index: print(&quot;--Examine Single Row--&quot;) print(df.loc[&quot;day1&quot;]) . -Examine Selected Rows calories duration day1 420 50 day3 390 45 --Examine Single Row-- calories 420 duration 50 Name: day1, dtype: int64 . Pandas DataFrame Information . print(df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 3 entries, day1 to day3 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 calories 3 non-null int64 1 duration 3 non-null int64 dtypes: int64(2) memory usage: 180.0+ bytes None . Example of larger data set . Pandas can read CSV and many other types of files, run the following code to see more features with a larger data set . import pandas as pd #read csv and sort &#39;Duration&#39; largest to smallest df = pd.read_csv(&#39;files/data.csv&#39;).sort_values(by=[&#39;Duration&#39;], ascending=False) print(&quot;--Duration Top 10&quot;) print(df.head(10)) print(&quot;--Duration Bottom 10&quot;) print(df.tail(10)) . --Duration Top 10 Duration Pulse Maxpulse Calories 69 300 108 143 1500.2 79 270 100 131 1729.0 109 210 137 184 1860.4 60 210 108 160 1376.0 106 180 90 120 800.3 90 180 101 127 600.1 65 180 90 130 800.4 61 160 110 137 1034.4 62 160 109 135 853.0 67 150 107 130 816.0 --Duration Bottom 10 Duration Pulse Maxpulse Calories 68 20 106 136 110.4 100 20 95 112 77.7 89 20 83 107 50.3 135 20 136 156 189.0 94 20 150 171 127.4 95 20 151 168 229.4 139 20 141 162 222.4 64 20 110 130 131.4 112 15 124 139 124.2 93 15 80 100 50.5 . APIs are a Source for Writing Programs with Data . 3rd Party APIs are a great source for creating Pandas Data Frames. . Data can be fetched and resulting json can be placed into a Data Frame | Observe output, this looks very similar to a Database | . &#39;&#39;&#39;Pandas can be used to analyze data&#39;&#39;&#39; import pandas as pd import requests def fetch(): &#39;&#39;&#39;Obtain data from an endpoint&#39;&#39;&#39; url = &quot;https://flask.nighthawkcodingsociety.com/api/covid/&quot; fetch = requests.get(url) json = fetch.json() # filter data for requirement df = pd.DataFrame(json[&#39;countries_stat&#39;]) # filter endpoint for country stats print(df.loc[0:5, &#39;country_name&#39;:&#39;deaths&#39;]) # show row 0 through 5 and columns country_name through deaths fetch() . country_name cases deaths 0 USA 82,649,779 1,018,316 1 India 43,057,545 522,193 2 Brazil 30,345,654 662,663 3 France 28,244,977 145,020 4 Germany 24,109,433 134,624 5 UK 21,933,206 173,352 . Hacks . Early Seed award . Add this Blog to you own Blogging site. | Have all lecture files saved to your files directory before Tech Talk starts. Have data.csv open in vscode. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In blog add College Board practice problems for 2.3. | . The next 4 weeks, Teachers want you to improve your understanding of data. Look at the blog and others on Unit 2. Your intention is to find some things to differentiate your individual College Board project. . Create or Find your own dataset. The suggestion is to use a JSON file, integrating with your PBL project would be Amazing. . | When choosing a data set, think about the following... . Does it have a good sample size? | Is there bias in the data? | Does the data set need to be cleaned? | What is the purpose of the data set? | ... | . | Continue this Blog using Pandas extract info from that dataset (ex. max, min, mean, median, mode, etc.) . | . Hack Helpers . Here is how Mort started on this assignment by asking ChatGPT ... Regarding Python Pandas, what are some data sets that would be good for learning Pandas? . There are many data sets that are suitable for learning pandas, depending on your interests and the skills you want to develop. . Here are some suggestions... . Titanic Dataset:This is a classic dataset for data analysis and machine learning, and is often used as an introduction to pandas. The dataset contains information about passengers on the Titanic, including their demographics, ticket class, and survival status. | Iris Dataset: This is another classic dataset that is often used in machine learning and data analysis courses. The dataset contains information about iris flowers, including their sepal length, sepal width, petal length, and petal width. . | NBA Player Stats: This dataset contains information about NBA players, including their stats for various games, their positions, and their salaries. It is a great dataset for learning how to clean and manipulate data using pandas. . | Housing Prices Dataset: This dataset contains information about housing prices in a particular city or region, including variables such as the number of bedrooms, square footage, and price. It is a great dataset for learning how to manipulate and analyze numerical data using pandas. . | COVID-19 Dataset: This dataset contains information about the COVID-19 pandemic, including the number of cases, deaths, and recoveries in various countries and regions. It is a great dataset for learning how to manipulate and analyze time-series data using pandas. . | World Bank Dataset: This dataset contains information about economic indicators from various countries, such as GDP, population, and poverty rates. It is a great dataset for learning how to manipulate and analyze large datasets using pandas. . | . | . These are just a few examples of the many datasets that are suitable for learning pandas. The best dataset for you will depend on your interests and the specific skills you want to develop. . Follow up question, &quot;where can I find Titanic data set?&quot; . | The Titanic dataset is a popular dataset for data analysis and machine learning, and it can be found on various websites and data repositories. Here are a few places where you can find the Titanic dataset... . Kaggle:The Titanic dataset is available on Kaggle, which is a popular platform for data scientists and machine learning enthusiasts. You can download the dataset from the Kaggle website after creating an account. | UCI Machine Learning Repository: The Titanic dataset is also available on the UCI Machine Learning Repository, which is a collection of datasets that are commonly used for machine learning research. You can download the dataset from the UCI Machine Learning Repository website. . | Seaborn library: If you have the Seaborn library installed in your Python environment, you can load the Titanic dataset directly from the library using the following code: . import seaborn as sns titanic_data = sns.load_dataset(&#39;titanic&#39;) . | . | . Titanic Data . Look at a sample of data. . import seaborn as sns # Load the titanic dataset titanic_data = sns.load_dataset(&#39;titanic&#39;) print(&quot;Titanic Data&quot;) print(titanic_data.columns) # titanic data set print(titanic_data[[&#39;survived&#39;,&#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;class&#39;, &#39;fare&#39;, &#39;embark_town&#39;]]) # look at selected columns . Titanic Data Index([&#39;survived&#39;, &#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;embarked&#39;, &#39;class&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;deck&#39;, &#39;embark_town&#39;, &#39;alive&#39;, &#39;alone&#39;], dtype=&#39;object&#39;) survived pclass sex age sibsp parch class fare 0 0 3 male 22.0 1 0 Third 7.2500 1 1 1 female 38.0 1 0 First 71.2833 2 1 3 female 26.0 0 0 Third 7.9250 3 1 1 female 35.0 1 0 First 53.1000 4 0 3 male 35.0 0 0 Third 8.0500 .. ... ... ... ... ... ... ... ... 886 0 2 male 27.0 0 0 Second 13.0000 887 1 1 female 19.0 0 0 First 30.0000 888 0 3 female NaN 1 2 Third 23.4500 889 1 1 male 26.0 0 0 First 30.0000 890 0 3 male 32.0 0 0 Third 7.7500 embark_town 0 Southampton 1 Cherbourg 2 Southampton 3 Southampton 4 Southampton .. ... 886 Southampton 887 Southampton 888 Southampton 889 Cherbourg 890 Queenstown [891 rows x 9 columns] . Use Pandas to clean the data. Most analysis, like Machine Learning or even Pandas in general like data to be in standardized format. This is called &#39;Training&#39; or &#39;Cleaning&#39; data. . # Preprocess the data from sklearn.preprocessing import OneHotEncoder td = titanic_data td.drop([&#39;alive&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;class&#39;, &#39;embark_town&#39;, &#39;deck&#39;], axis=1, inplace=True) td.dropna(inplace=True) td[&#39;sex&#39;] = td[&#39;sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) td[&#39;alone&#39;] = td[&#39;alone&#39;].apply(lambda x: 1 if x == True else 0) # Encode categorical variables enc = OneHotEncoder(handle_unknown=&#39;ignore&#39;) enc.fit(td[[&#39;embarked&#39;]]) onehot = enc.transform(td[[&#39;embarked&#39;]]).toarray() cols = [&#39;embarked_&#39; + val for val in enc.categories_[0]] td[cols] = pd.DataFrame(onehot) td.drop([&#39;embarked&#39;], axis=1, inplace=True) td.dropna(inplace=True) print(td) . survived pclass sex age sibsp parch fare alone embarked_C 0 0 3 1 22.0 1 0 7.2500 0 0.0 1 1 1 0 38.0 1 0 71.2833 0 1.0 2 1 3 0 26.0 0 0 7.9250 1 0.0 3 1 1 0 35.0 1 0 53.1000 0 0.0 4 0 3 1 35.0 0 0 8.0500 1 0.0 .. ... ... ... ... ... ... ... ... ... 705 0 2 1 39.0 0 0 26.0000 1 0.0 706 1 2 0 45.0 0 0 13.5000 1 0.0 707 1 1 1 42.0 0 0 26.2875 1 0.0 708 1 1 0 22.0 0 0 151.5500 1 0.0 710 1 1 0 24.0 0 0 49.5042 1 1.0 embarked_Q embarked_S 0 0.0 1.0 1 0.0 0.0 2 0.0 1.0 3 0.0 1.0 4 0.0 1.0 .. ... ... 705 0.0 1.0 706 0.0 1.0 707 1.0 0.0 708 0.0 1.0 710 0.0 0.0 [564 rows x 11 columns] . The result of &#39;Training&#39; data is making it easier to analyze or make conclusions. In looking at the Titanic, as you clean you would probably want to make assumptions on likely chance of survival. . This would involve analyzing various factors (such as age, gender, class, etc.) that may have affected a person&#39;s chances of survival, and using that information to make predictions about whether an individual would have survived or not. . Data description:- Survival - Survival (0 = No; 1 = Yes). Not included in test.csv file. - Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) . Name - Name | Sex - Sex | Age - Age | Sibsp - Number of Siblings/Spouses Aboard | Parch - Number of Parents/Children Aboard | Ticket - Ticket Number | Fare - Passenger Fare | Cabin - Cabin | Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) | . | Perished Mean/Average . | . print(titanic_data.query(&quot;survived == 0&quot;).mean()) . survived 0.000000 pclass 2.464072 sex 0.844311 age 31.073353 sibsp 0.562874 parch 0.398204 fare 24.835902 alone 0.616766 embarked_C 0.185629 embarked_Q 0.038922 embarked_S 0.775449 dtype: float64 . Survived Mean/Average | . print(td.query(&quot;survived == 1&quot;).mean()) . survived 1.000000 pclass 1.878261 sex 0.326087 age 28.481522 sibsp 0.504348 parch 0.508696 fare 50.188806 alone 0.456522 embarked_C 0.152174 embarked_Q 0.034783 embarked_S 0.813043 dtype: float64 . Survived Max and Min Stats . print(td.query(&quot;survived == 1&quot;).max()) print(td.query(&quot;survived == 1&quot;).min()) . survived 1.0000 pclass 3.0000 sex 1.0000 age 80.0000 sibsp 4.0000 parch 5.0000 fare 512.3292 alone 1.0000 embarked_C 1.0000 embarked_Q 1.0000 embarked_S 1.0000 dtype: float64 survived 1.00 pclass 1.00 sex 0.00 age 0.75 sibsp 0.00 parch 0.00 fare 0.00 alone 0.00 embarked_C 0.00 embarked_Q 0.00 embarked_S 0.00 dtype: float64 . Machine Learning Visit Tutorials Point . Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. . Description from ChatGPT. The Titanic dataset is a popular dataset for data analysis and machine learning. In the context of machine learning, accuracy refers to the percentage of correctly classified instances in a set of predictions. In this case, the testing data is a subset of the original Titanic dataset that the decision tree model has not seen during training......After training the decision tree model on the training data, we can evaluate its performance on the testing data by making predictions on the testing data and comparing them to the actual outcomes. The accuracy of the decision tree classifier on the testing data tells us how well the model generalizes to new data that it hasn&#39;t seen before......For example, if the accuracy of the decision tree classifier on the testing data is 0.8 (or 80%), this means that 80% of the predictions made by the model on the testing data were correct....Chance of survival could be done using various machine learning techniques, including decision trees, logistic regression, or support vector machines, among others. . | Code Below prepares data for further analysis and provides an Accuracy. IMO, you would insert a new passenger and predict survival. Datasets could be used on various factors like prediction if a player will hit a Home Run, or a Stock will go up or down. . Decision Trees, prediction by a piecewise constant approximation. | Logistic Regression, the probabilities describing the possible outcomes. | . | . from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Split arrays or matrices into random train and test subsets. X = td.drop(&#39;survived&#39;, axis=1) y = td[&#39;survived&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a decision tree classifier dt = DecisionTreeClassifier() dt.fit(X_train, y_train) # Test the model y_pred = dt.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;DecisionTreeClassifier Accuracy:&#39;, accuracy) # Train a logistic regression model logreg = LogisticRegression() logreg.fit(X_train, y_train) # Test the model y_pred = logreg.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;LogisticRegression Accuracy:&#39;, accuracy) . DecisionTreeClassifier Accuracy: 0.7705882352941177 LogisticRegression Accuracy: 0.788235294117647 . /Users/johnmortensen/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/03/09/AP-unit2-3.html",
            "relUrl": "/2023/03/09/AP-unit2-3.html",
            "date": " • Mar 9, 2023"
        }
        
    
  
    
        ,"post15": {
            "title": "Unit 2.2 Data Compression, Images",
            "content": "Enumerate &quot;Data&quot; Big Idea from College Board . Some of the big ideas and vocab that you observe, talk about it with a partner ... . &quot;Data compression is the reduction of the number of bits needed to represent data&quot; | &quot;Data compression is used to save transmission time and storage space.&quot; | &quot;lossy data can reduce data but the original data is not recovered&quot; | &quot;lossless data lets you restore and recover&quot; | . The Image Lab Project contains a plethora of College Board Unit 2 data concepts. Working with Images provides many opportunities for compression and analyzing size. . Image Files and Size . Here are some Images Files. Download these files, load them into images directory under _notebooks in your Blog. . Clouds Impression | Lassen Volcano | Green Square | . Describe some of the meta data and considerations when managing Image files. Describe how these relate to Data Compression ... . File Type, PNG and JPG are two types used in this lab | Size, height and width, number of pixels | Visual perception, lossy compression | . Displaying images in Python Jupyter notebook . Python Libraries and Concepts used for Jupyter and Files/Directories . IPython . Support visualization of data in Jupyter notebooks. Visualization is specific to View, for the web visualization needs to be converted to HTML. . pathlib . File paths are different on Windows versus Mac and Linux. This can cause problems in a project as you work and deploy on different Operating Systems (OS&#39;s), pathlib is a solution to this problem. . What are commands you use in terminal to access files? | What are the command you use in Windows terminal to access files? | What are some of the major differences? | . Provide what you observed, struggled with, or leaned while playing with this code. . Why is path a big deal when working with images? | How does the meta data source and label relate to Unit 5 topics? | Look up IPython, describe why this is interesting in Jupyter Notebooks for both Pandas and Images? | . from IPython.display import Image, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images def image_display(images): for image in images: display(Image(filename=image[&#39;filename&#39;])) # Run this as standalone tester to see sample data printed in Jupyter terminal if __name__ == &quot;__main__&quot;: # print parameter supplied image green_square = image_data(images=[{&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}]) image_display(green_square) # display default images from image_data() default_images = image_data() image_display(default_images) . Reading and Encoding Images (2 implementations follow) . PIL (Python Image Library) . Pillow or PIL provides the ability to work with images in Python. Geeks for Geeks shows some ideas on working with images. . base64 . Image formats (JPG, PNG) are often called *Binary File formats, it is difficult to pass these over HTTP. Thus, base64 converts binary encoded data (8-bit, ASCII/Unicode) into a text encoded scheme (24 bits, 6-bit Base64 digits). Thus base64 is used to transport and embed binary images into textual assets such as HTML and CSS.- How is Base64 similar or different to Binary and Hexadecimal? . Translate first 3 letters of your name to Base64. | . numpy . Numpy is described as &quot;The fundamental package for scientific computing with Python&quot;. In the Image Lab, a Numpy array is created from the image data in order to simplify access and change to the RGB values of the pixels, converting pixels to grey scale. . io, BytesIO . Input and Output (I/O) is a fundamental of all Computer Programming. Input/output (I/O) buffering is a technique used to optimize I/O operations. In large quantities of data, how many frames of input the server currently has queued is the buffer. In this example, there is a very large picture that lags. . Where have you been a consumer of buffering? | From your consumer experience, what effects have you experienced from buffering? | How do these effects apply to images? | . Data Structures, Imperative Programming Style, and working with Images . Introduction to creating meta data and manipulating images. Look at each procedure and explain the the purpose and results of this program. Add any insights or challenges as you explored this program. . Does this code seem like a series of steps are being performed? | Describe Grey Scale algorithm in English or Pseudo code? | Describe scale image? What is before and after on pixels in three images? | Is scale image a type of compression? If so, line it up with College Board terms described? | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images # Large image scaled to baseWidth of 320 def scale_image(img): baseWidth = 320 scalePercent = (baseWidth/float(img.size[0])) scaleHeight = int((float(img.size[1])*float(scalePercent))) scale = (baseWidth, scaleHeight) return img.resize(scale) # PIL image converted to base64 def image_to_base64(img, format): with BytesIO() as buffer: img.save(buffer, format) return base64.b64encode(buffer.getvalue()).decode() # Set Properties of Image, Scale, and convert to Base64 def image_management(image): # path of static images is defaulted # Image open return PIL image object img = pilImage.open(image[&#39;filename&#39;]) # Python Image Library operations image[&#39;format&#39;] = img.format image[&#39;mode&#39;] = img.mode image[&#39;size&#39;] = img.size # Scale the Image img = scale_image(img) image[&#39;pil&#39;] = img image[&#39;scaled_size&#39;] = img.size # Scaled HTML image[&#39;html&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(image[&#39;pil&#39;], image[&#39;format&#39;]) # Create Grey Scale Base64 representation of Image def image_management_add_html_grey(image): # Image open return PIL image object img = image[&#39;pil&#39;] format = image[&#39;format&#39;] img_data = img.getdata() # Reference https://www.geeksforgeeks.org/python-pil-image-getdata/ image[&#39;data&#39;] = np.array(img_data) # PIL image to numpy array image[&#39;gray_data&#39;] = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in image[&#39;data&#39;]: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: image[&#39;gray_data&#39;].append((average, average, average, pixel[3])) # PNG format else: image[&#39;gray_data&#39;].append((average, average, average)) # end for loop for pixels img.putdata(image[&#39;gray_data&#39;]) image[&#39;html_grey&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(img, format) # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: # Use numpy to concatenate two arrays images = image_data() # Display meta data, scaled view, and grey scale for each image for image in images: image_management(image) print(&quot;- meta data --&quot;) print(image[&#39;label&#39;]) print(image[&#39;source&#39;]) print(image[&#39;format&#39;]) print(image[&#39;mode&#39;]) print(&quot;Original size: &quot;, image[&#39;size&#39;]) print(&quot;Scaled size: &quot;, image[&#39;scaled_size&#39;]) print(&quot;-- original image --&quot;) display(HTML(image[&#39;html&#39;])) print(&quot; grey image -&quot;) image_management_add_html_grey(image) display(HTML(image[&#39;html_grey&#39;])) print() . Data Structures and OOP . Most data structures classes require Object Oriented Programming (OOP). Since this class is lined up with a College Course, OOP will be talked about often. Functionality in remainder of this Blog is the same as the prior implementation. Highlight some of the key difference you see between imperative and oop styles. . Read imperative and object-oriented programming on Wikipedia | Consider how data is organized in two examples, in relations to procedures | Look at Parameters in Imperative and Self in OOP | . Additionally, review all the imports in these three demos. Create a definition of their purpose, specifically these ... . PIL | numpy | base64 | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_grey = self.image_to_html_grey() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_grey(self): return self._html_grey # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Grey Scale Base64 representation of Image def image_to_html_grey(self): img_grey = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array grey_data = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in numpy: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: grey_data.append((average, average, average, pixel[3])) # PNG format else: grey_data.append((average, average, average)) # end for loop for pixels img_grey.putdata(grey_data) return self.image_to_html(img_grey) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) print(&quot; grey image &quot;) display(HTML(ido.html_grey)) print() . Hacks . Early Seed award . Add this Blog to you own Blogging site. | In the Blog add a Happy Face image. | Have Happy Face Image open when Tech Talk starts, running on localhost. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . In the Blog add notes and observations on each code cell that request an answer. | In blog add College Board practice problems for 2.2 | Choose 2 images, one that will more likely result in lossy data compression and one that is more likely to result in lossless data compression. Explain. | . Project Addition . If your project has images in it, try to implement an image change that has a purpose. (Ex. An item that has been sold out could become gray scale) | . Pick a programming paradigm and solve some of the following ... . Numpy, manipulating pixels. As opposed to Grey Scale treatment, pick a couple of other types like red scale, green scale, or blue scale. We want you to be manipulating pixels in the image. | Binary and Hexadecimal reports. Convert and produce pixels in binary and Hexadecimal and display. | Compression and Sizing of images. Look for insights into compression Lossy and Lossless. Look at PIL library and see if there are other things that can be done. | There are many effects you can do as well with PIL. Blur the image or write Meta Data on screen, aka Title, Author and Image size. | .",
            "url": "https://paravsalaniwal.github.io/T3Project/2023/03/07/AP-unit2-2.html",
            "relUrl": "/2023/03/07/AP-unit2-2.html",
            "date": " • Mar 7, 2023"
        }
        
    
  

  
  
      ,"page0": {
          "title": "Project",
          "content": ". Pick a Team Atlanta Hawks Boston Celtics Brooklyn Nets Charlotte Hornets Chicago Bulls Cleveland Cavaliers Dallas Mavericks Denver Nuggets Detroit Pistons Golden State Warriors Houton Rockets Indiana Pacers Los Angeles Clippers Los Angeles Lakers Memphis Grizzlies Miami Heat Milwaukee Bucks Minnesota Timberwolves New Orleans Pelicans New York Knicks Oklahoma City Thunder Orlando Magic Philadelphia 76ers Phoenix Suns Portland Trail Blazers Sacramento Kings San Antonio Spurs Toronto Raptors Utah Jazz Washington Wizards Pick a Stat - Pie Graph Points Assists Rebounds Steals Blocks Field Goal % 3 Point % Pick a Stat - Bar Graph Points Assists Rebounds Steals Blocks Field Goal % 3 Point % .",
          "url": "https://paravsalaniwal.github.io/T3Project/Project/",
          "relUrl": "/Project/",
          "date": ""
      }
      
  

  

  
      ,"page2": {
          "title": "About",
          "content": "This site is built with fastpages .",
          "url": "https://paravsalaniwal.github.io/T3Project/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
      ,"page5": {
          "title": "Data Analysis Project",
          "content": "Click the table headers to sort the entries in descending order. Click the table headers again to reverse the order. . Team Name Year Founded Championships Won . Atlanta Hawks | 1949 | 01 | . Boston Celtics | 1946 | 17 | . Brooklyn Nets | 1976 | 00 | . Charlotte Hornets | 1988 | 00 | . Chicago Bulls | 1966 | 06 | . Cleveland Cavaliers | 1970 | 01 | . Dallas Mavericks | 1980 | 01 | . Denver Nuggets | 1976 | 00 | . Detroit Pistons | 1948 | 03 | . Golden State Warriors | 1946 | 07 | . Houston Rockets | 1967 | 02 | . Indiana Pacers | 1976 | 00 | . Los Angeles Clippers | 1970 | 00 | . Los Angeles Lakers | 1948 | 17 | . Memphis Grizzlies | 1995 | 00 | . Miami Heat | 1988 | 03 | . Milwaukee Bucks | 1968 | 02 | . Minnesota Timberwolves | 1989 | 00 | . New Orleans Pelicans | 2002 | 00 | . New York Knicks | 1946 | 02 | . Oklahoma City Thunder | 1967 | 01 | . Orlando Magic | 1989 | 00 | . Philadelphia 76ers | 1949 | 03 | . Phoenix Suns | 1968 | 00 | . Portland Trail Blazers | 1970 | 01 | . Sacramento Kings | 1948 | 00 | . San Antonio Spurs | 1976 | 05 | . Toronto Raptors | 1995 | 01 | . Utah Jazz | 1974 | 00 | . Washington Wizards | 1961 | 01 | . .",
          "url": "https://paravsalaniwal.github.io/T3Project/DataAnalysisProject/",
          "relUrl": "/DataAnalysisProject/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
  

  
  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://paravsalaniwal.github.io/T3Project/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}